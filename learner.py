import numpy as np
import torch
from rlcore.algo import IPPO, JointPPO
from rlagent import Neo
from mpnn import MPNN
from utils import make_multiagent_env


def setup_master(args, env=None, return_env=False):
    if env is None:
        env = make_multiagent_env(args.env_name, num_agents=args.num_agents, dist_threshold=args.dist_threshold, 
                                  arena_size=args.arena_size, identity_size=args.identity_size,
                                  mask_obs_dist=args.mask_obs_dist if hasattr(args, 'mask_obs_dist') else None)
    policy1 = None
    policy2 = None
    team1 = []
    team2 = []

    num_adversary = 0
    num_friendly = 0
    for i,agent in enumerate(env.world.policy_agents):
        if hasattr(agent, 'adversary') and agent.adversary:
            num_adversary += 1
        else:
            num_friendly += 1

    # share a common policy in a team
    action_space = env.action_space[i]
    entity_mp = args.entity_mp
    if args.env_name == 'simple_spread':
        num_entities = args.num_agents
    elif args.env_name == 'simple_formation':
        num_entities = 1
    elif args.env_name == 'simple_line':
        num_entities = 2
    else:
        raise NotImplementedError('Unknown environment, define entity_mp for this!')

    if entity_mp:
        pol_obs_dim = env.observation_space[i].shape[0] - 2*(2*num_entities-1)
    else:
        pol_obs_dim = env.observation_space[i].shape[0]
    
    '''
    print("action_space: ", action_space)
    print("observation_space:", env.observation_space[i].shape[0])
    print("pol_obs_dim: ", pol_obs_dim)
    '''

    # index at which agent's position is present in its observation
    pos_index = args.identity_size + 2
    for i, agent in enumerate(env.world.policy_agents):
        obs_dim = env.observation_space[i].shape[0]

        if hasattr(agent, 'adversary') and agent.adversary:
            if policy1 is None:
                policy1 = MPNN(input_size=pol_obs_dim,num_agents=num_adversary,num_entities=num_entities,action_space=action_space,
                               pos_index=pos_index, mask_dist=args.mask_dist,entity_mp=entity_mp, is_recurrent=args.is_recurrent).to(args.device)
            team1.append(Neo(args,policy1,(obs_dim,),action_space))
        else:
            if policy2 is None:
                policy2 = MPNN(input_size=pol_obs_dim,num_agents=num_friendly,num_entities=num_entities,action_space=action_space,
                               pos_index=pos_index, mask_dist=args.mask_dist,mask_obs_dist=args.mask_obs_dist,entity_mp=entity_mp,is_recurrent=args.is_recurrent).to(args.device)

            team2.append(Neo(args,policy2,(obs_dim,),action_space))

        # ================== æ–°å¢ï¼šåŠ è½½é¢„è®­ç»ƒåº•å±‚ç½‘ç»œ ==================
        # å‡è®¾ä½ åœ¨ args ä¸­å®šä¹‰äº† load_low_level_path å’Œ freeze_low_level
        if hasattr(args, 'load_low_level_path') and args.load_low_level_path is not None:
            print(f"ğŸ”„ Loading pretrained low-level model from: {args.load_low_level_path}")
            
            # å¦‚æœæœ‰ policy1 (Adversary)ï¼ŒåŠ è½½
            if policy1 is not None:
                policy1.load_pretrained_low_level(args.load_low_level_path, freeze=True) # å»ºè®®é»˜è®¤å†»ç»“
                
            # å¦‚æœæœ‰ policy2 (Friendly)ï¼ŒåŠ è½½
            if policy2 is not None:
                policy2.load_pretrained_low_level(args.load_low_level_path, freeze=True)
        
        # ================== æ–°å¢ï¼šåŠ è½½é¢„è®­ç»ƒé«˜å±‚ç½‘ç»œ ==================
        if hasattr(args, 'load_high_level_path') and args.load_high_level_path is not None:
            print(f"ğŸ”„ Loading pretrained high-level model from: {args.load_high_level_path}")
            
            if policy1 is not None:
                policy1.load_pretrained_high_level(args.load_high_level_path, freeze=False)
            if policy2 is not None:
                policy2.load_pretrained_high_level(args.load_high_level_path, freeze=False)
        
        if hasattr(args, 'load_high_critic_path') and args.load_high_critic_path is not None:
            print(f"ğŸ”„ Loading pretrained high-level critic from: {args.load_high_critic_path}")
            
            if policy1 is not None:
                policy1.load_pretrained_high_level(args.load_high_critic_path, freeze=False)
            if policy2 is not None:
                policy2.load_pretrained_high_level(args.load_high_critic_path, freeze=False)
        # ============================================================
        
    master = Learner(args, [team1, team2], [policy1, policy2], env=env) # ä¼ å…¥å¹¶è¡Œç¯å¢ƒ
    
    if args.continue_training:
        print("Loading pretrained model")
        master.load_models(torch.load(args.load_dir, map_location=torch.device('cpu'))['models'])

    if return_env:
        return master, env
    return master


class Learner(object):
    def __init__(self, args, teams_list, policies_list, env):
        self.teams_list = [x for x in teams_list if len(x)!=0]
        self.all_agents = [agent for team in teams_list for agent in team]
        self.policies_list = [x for x in policies_list if x is not None]
        # self.trainers_list = [JointPPO(policy, args.clip_param, args.ppo_epoch, args.num_mini_batch, args.value_loss_coef,
        #                                args.entropy_coef, lr=args.lr, max_grad_norm=args.max_grad_norm,
        #                                use_clipped_value_loss=args.clipped_value_loss) for policy in self.policies_list]
        # â­ æ£€æµ‹æ˜¯å¦åŠ è½½äº†é¢„è®­ç»ƒåº•å±‚ç½‘ç»œ
        self.use_pretrained_low_level = (
            hasattr(args, 'load_low_level_path') and 
            args.load_low_level_path is not None
        )

        # åˆå§‹åŒ–è®­ç»ƒå™¨
        if not self.use_pretrained_low_level:
            self.trainers_list = [IPPO(policy, args.clip_param, args.ppo_epoch, args.num_mini_batch, args.value_loss_coef,
                                        args.entropy_coef, lr=args.lr, max_grad_norm=args.max_grad_norm,
                                        use_clipped_value_loss=args.clipped_value_loss) for policy in self.policies_list]
        self.high_trainers_list = [JointPPO(policy, args.clip_param, args.ppo_epoch, args.num_mini_batch, args.value_loss_coef,
                                       args.entropy_coef, lr=args.lr, max_grad_norm=args.max_grad_norm,
                                       use_clipped_value_loss=args.clipped_value_loss) for policy in self.policies_list]
        self.device = args.device
        self.env = env
        self.envs_info = None
        self.high_level_interval = args.high_level_interval

        self.top_k = args.top_k
        self.rrt_max_iter = args.rrt_max_iter

    @property
    def all_policies(self):
        return [agent.actor_critic.state_dict() for agent in self.all_agents]

    @property
    def team_attn(self):
        return self.policies_list[0].attn_mat

    def initialize_obs(self, obs):
        # obs - num_processes x num_agents x obs_dim
        for i, agent in enumerate(self.all_agents):
            agent.initialize_obs(torch.from_numpy(obs[:,i,:]).float().to(self.device))
            agent.rollouts.to(self.device)

    def initialize_env_state(self, env_state):
        # obs - num_processes x num_agents x obs_dim
        for i, agent in enumerate(self.all_agents):
            agent.initialize_env_state(torch.from_numpy(env_state).float().to(self.device))
            agent.rollouts.to(self.device)

    def act(self, step):
        # æ ¹æ®å½“å‰çš„ç¯å¢ƒä¸­æ™ºèƒ½ä½“çš„çŠ¶æ€è¿›è¡Œå†³ç­–ï¼Œç”Ÿæˆä¸‹ä¸€æ­¥çš„åŠ¨ä½œåˆ—è¡¨ï¼Œè¿™é‡Œå¯ä»¥ç›´æ¥å½“ä½œåˆ†å±‚ç½‘ç»œçš„æ€»å¯ï¼Œä¸‹é¢å†è¿›è¡Œç»†åˆ†æ˜¯highè¿˜æ˜¯low
        actions_list = []
        goals_list = []
        tasks_list = []
        for team, policy in zip(self.teams_list, self.policies_list):
            # concatenate all inputs
            all_obs = torch.cat([agent.rollouts.obs[step] for agent in team])
            all_hidden = torch.cat([agent.rollouts.recurrent_hidden_states[step] for agent in team])
            all_masks = torch.cat([agent.rollouts.masks[step] for agent in team])
            all_env_state = torch.cat([agent.rollouts.env_states[step] for agent in team]) # å®é™…ä¸Šåªéœ€è¦å…¶ä¸­ä¸€ä¸ªagentçš„ç¯å¢ƒçŠ¶æ€å°±å¤Ÿäº†ï¼Œå³all_env_state[:num_processes]

            # é»˜è®¤é‡‡å–ä¹‹å‰çš„ç›®æ ‡åˆ†é…ï¼Œ[num_agents * num_processes, 2]ï¼Œä»¥åŠä»»åŠ¡ç±»å‹[num_agents * num_processes, 1]
            # å³ä½¿step=0æ—¶ï¼Œä¹Ÿå¤åˆ¶goals[-1]çš„å€¼ï¼Œä¿è¯goalæœ‰æ•ˆ
            all_goals = torch.cat([agent.rollouts.goals[step-1] for agent in team])
            all_tasks = torch.cat([agent.rollouts.tasks[step-1] for agent in team])
            all_higoal_log_probs = torch.cat([agent.rollouts.higoal_log_probs[step-1] for agent in team])
            all_landmark_datas = torch.cat([agent.rollouts.landmark_datas[step-1] for agent in team])
            all_landmark_masks = torch.cat([agent.rollouts.landmark_masks[step-1] for agent in team])

            # å‘é‡åŒ–ä¼˜åŒ–=================
            # è¾“å…¥æå–
            ## æ‰¹é‡å¤„ç†æ‰€æœ‰ç¯å¢ƒ
            num_processes = len(self.envs_info)
            num_agents = len(team)

            # ä»all_masksä¸­æå–env_doneä¿¡æ¯ï¼Œå…¨0è¡¨ç¤ºå½“å‰processçš„episodeç»“æŸ
            episode_dones = all_masks.view(num_agents, num_processes).transpose(0, 1)
            env_dones = (episode_dones.sum(dim=1) == 0)  # [num_processes]ï¼ŒTrueè¡¨ç¤ºè¯¥processçš„episodeç»“æŸ

            # â­ æ„å»ºCriticè¾“å…¥
            entropy_maps = torch.stack([torch.from_numpy(np.array(info['entropy_map'])).float() 
                                        for info in self.envs_info]).to(self.device)  # [num_processes, H, W]
            heatmaps = torch.stack([torch.from_numpy(np.array(info['heatmap'])).float() 
                                    for info in self.envs_info]).to(self.device)  # [num_processes, H, W]
            landmark_heatmaps = torch.stack([torch.from_numpy(np.array(info['landmark_heatmap'])).float() 
                                            for info in self.envs_info]).to(self.device)  # [num_processes, H, W]
            all_critic_map_inp = torch.stack([entropy_maps, heatmaps, landmark_heatmaps], dim=1)  # [num_processes, 3, H, W]
            
            # â­ æ”¶é›†goal_doneå’Œbatteryä¿¡æ¯
            goal_done_list = [info['goal_done'] for info in self.envs_info]
            goal_done_mask = torch.tensor(goal_done_list, dtype=torch.bool, device=self.device)
            agent_world_steps = torch.tensor(
                [info['world_steps'] for info in self.envs_info], 
                dtype=torch.float32, 
                device=self.device
            ).unsqueeze(1).repeat(1, num_agents).unsqueeze(-1)
            agent_batterys = (50.0 - agent_world_steps) / 50.0

            # â­ ç”ŸæˆlandmarkèŠ‚ç‚¹
            detected_maps = [torch.from_numpy(np.array(info['map'][1])).float().to(self.device) 
                            for info in self.envs_info] # num_processes list of Tensor [num_detected, 2]
            new_detected, new_detected_masks = self.update_landmark_info(all_landmark_datas, 
                                                                         all_landmark_masks, 
                                                                         detected_maps, 
                                                                         self.device, 
                                                                         env_dones)
            # new_detected: Tensor shapeÂ [num_agents * num_processes, max_landmarks, 4], agent first
            # new_detected_masks: Tensor shapeÂ [num_agents * num_processes, max_landmarks, 1], agent first


            # â­ å‡†å¤‡æ™ºèƒ½ä½“èŠ‚ç‚¹æ•°æ®
            agent_entropy_maps = entropy_maps.unsqueeze(1).repeat(1, num_agents, 1, 1)
            voronoi_masks = torch.stack([
                torch.stack([torch.from_numpy(np.array(info['voronoi_masks'][a])).float() 
                            for a in range(num_agents)]) 
                for info in self.envs_info
            ]).to(self.device)
            agent_positions = all_obs[:, 2:4].view(num_agents, num_processes, 2).transpose(0, 1)
            agent_vels = all_obs[:, 0:2].view(num_agents, num_processes, 2).transpose(0, 1)
            agent_goals = all_goals.view(num_agents, num_processes, 2).transpose(0, 1)
            agent_nodes = torch.cat([agent_positions, agent_goals], dim=-1)
            ego_nodes = torch.cat([agent_positions, agent_vels, agent_batterys], dim=-1)
            # è®¡ç®—æ™ºèƒ½ä½“åˆ°ç›®æ ‡çš„æ¬§å¼è·ç¦»
            dist_to_goal = torch.norm(agent_goals - agent_positions, dim=-1, keepdim=True)  # [num_processes, num_agents, 1]
            # æ‹¼æ¥å½¢æˆé˜Ÿå‹èŠ‚ç‚¹ç‰¹å¾ [num_processes, num_agents, 5]
            teammate_nodes = torch.cat([
                agent_positions,  # [num_processes, num_agents, 2]
                agent_vels,       # [num_processes, num_agents, 2]
                dist_to_goal            # [num_processes, num_agents, 1]
            ], dim=-1)  # [num_processes, num_agents, 5]

            # â­ å‡†å¤‡å…¨å±€é˜Ÿå‹æ©ç  (åŸºäºæ™ºèƒ½ä½“æ˜¯å¦å­˜æ´»)
            global_teammate_mask = all_masks.view(num_agents, num_processes).t().unsqueeze(-1) # [P, A, 1]

            # landmark node docker: Tensor shapeÂ [num_agents * num_processes, Max_L, 4]
            all_landmark_nodes = torch.zeros(num_processes * num_agents, new_detected.shape[1], 4, device=self.device)

            # â­ é«˜å±‚å†³ç­–ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if goal_done_mask.any():
                update_indices = torch.nonzero(goal_done_mask, as_tuple=False)
                proc_indices = update_indices[:, 0]
                agent_indices = update_indices[:, 1]
                
                map_inps = torch.stack([
                    agent_entropy_maps[proc_indices],
                    voronoi_masks[proc_indices]
                ], dim=0)
                vec_inp_agents = agent_nodes[proc_indices] # [N, A, 4]

                # â­ ç”Ÿæˆé˜Ÿå‹èŠ‚ç‚¹
                # é˜Ÿå‹èŠ‚ç‚¹ï¼šTensor shapeÂ [N, num_agents, 5]ï¼ŒåŒ…å«ä½ç½®ï¼Œé€Ÿåº¦ï¼Œè·ç¦»ç›®æ ‡çš„è·ç¦»
                # é˜Ÿå‹èŠ‚ç‚¹ maskï¼š Tensor shapeÂ [N, num_agents, 1]ï¼Œ1è¡¨ç¤ºæœ‰æ•ˆï¼Œ0è¡¨ç¤ºæ— æ•ˆï¼Œæ’é™¤è‡ªå·±ï¼Œä»¥åŠepisode doneçš„æ™ºèƒ½ä½“
                batch_teammate_nodes = teammate_nodes[proc_indices]  # [N, num_agents, 5]

                # â­ ç”Ÿæˆæœ¬æ¬¡å†³ç­–æ‰€éœ€çš„ batch_teammate_masks [N, A, 1]
                # 1. ä»å…¨å±€æ©ç ä¸­æå–å¯¹åº”ç¯å¢ƒçš„æ©ç 
                batch_teammate_masks = global_teammate_mask[proc_indices].clone() # [N, A, 1]
                # 2. æ’é™¤è‡ªå·± (self-masking)
                batch_indices = torch.arange(len(proc_indices), device=self.device)
                batch_teammate_masks[batch_indices, agent_indices, 0] = 0.0
                
                # â­ RRTç”Ÿæˆæ¢ç´¢èŠ‚ç‚¹ï¼Œ# [B_pro(N), B_agents(1), K, 4]
                batch_explore_nodes = policy.get_explore_nodes(self.top_k, self.rrt_max_iter, vec_inp_agents, map_inps, agent_indices)
                batch_explore_nodes = batch_explore_nodes.reshape(-1, batch_explore_nodes.shape[-2], batch_explore_nodes.shape[-1])
                # batch_explore_nodes: Tensor shapeÂ [N, K, 4]
                
                # â­ è·å–landmarkèŠ‚ç‚¹
                batch_ego_nodes = ego_nodes[proc_indices, agent_indices]
                linear_indices = agent_indices * num_processes + proc_indices
                batch_landmark_nodes, batch_landmark_node_masks = policy.get_landmark_nodes(
                    all_obs[:,2:4], new_detected, new_detected_masks, linear_indices, all_masks
                )

                # save to all_landmark_nodes docker
                all_landmark_nodes[linear_indices] = batch_landmark_nodes   # not update is_detected
                
                # â­ è·å–è¾¹ç‰¹å¾
                batch_ego_to_explore_edges, batch_ego_to_landmark_edges, batch_ego_to_landmark_edge_masks = policy.get_edge_features(
                    batch_explore_nodes, batch_landmark_nodes, batch_landmark_node_masks
                )

                # â­ é«˜å±‚ç­–ç•¥å†³ç­–
                batch_goals = policy.get_high_level_goal(
                    batch_ego_nodes, # Tensor shapeÂ [N, 5]
                    batch_teammate_nodes, # Tensor shapeÂ [N, num_agents, 5]
                    batch_teammate_masks, # Tensor shapeÂ [N, num_agents, 1]
                    batch_explore_nodes, # Tensor shapeÂ [N, K, 4]
                    batch_ego_to_explore_edges, # TensorÂ [N, K, 3], length N
                    batch_landmark_nodes, # tensor: [N, Max_L, 4]
                    batch_landmark_node_masks, # tensor: [N, Max_L, 1]
                    batch_ego_to_landmark_edges, # Tensor [N, Max_L, 3]
                    batch_ego_to_landmark_edge_masks # Tensor [N, Max_L, 1]
                )
                
                # â­ æ›´æ–°ç›®æ ‡å’Œlandmark
                all_goals[linear_indices] = batch_goals["waypoints"]
                all_tasks[linear_indices] = batch_goals["action_modes"]
                all_higoal_log_probs[linear_indices] = batch_goals["node_log_probs"]
                
                for i, lin_idx in enumerate(linear_indices):
                    if batch_goals["action_modes"][i, 0] == 1:  # é€‰æ‹©çš„æ˜¯ landmark
                        # è·å–é€‰ä¸­çš„ waypointï¼ˆç»å¯¹ä¸–ç•Œåæ ‡ï¼‰
                        selected_waypoint = batch_goals["waypoints"][i]  # [2]
                        
                        # åœ¨å¯¹åº”çš„ landmark åˆ—è¡¨ä¸­æ‰¾åˆ°åŒ¹é…çš„ landmark
                        landmarks = new_detected[lin_idx]  # [max_landmarks, 4]
                        landmark_mask = new_detected_masks[lin_idx]  # [max_landmarks, 1]
                        
                        # æ‰¾åˆ°æœ‰æ•ˆçš„ landmark
                        valid_mask = landmark_mask[:, 0] > 0.5
                        if valid_mask.any():
                            # è®¡ç®—è·ç¦»ï¼ˆä½¿ç”¨ç»å¯¹åæ ‡ï¼‰
                            landmark_positions = landmarks[:, :2]  # [max_landmarks, 2]
                            distances = torch.norm(landmark_positions - selected_waypoint, dim=1)  # [max_landmarks]
                            distances = distances.masked_fill(~valid_mask, float('inf'))
                            
                            # æ‰¾åˆ°æœ€è¿‘çš„ landmark
                            min_idx = distances.argmin()
                            if distances[min_idx] < 0.05:  # åŒ¹é…é˜ˆå€¼ 0.05
                                new_detected[:, min_idx, 3] = 1.0  # è®¾ç½® is_targeted = 1
                                # â­ ä¿®å¤ï¼šåªæ›´æ–°å½“å‰ç¯å¢ƒ(process)ä¸­æ‰€æœ‰æ™ºèƒ½ä½“çš„ landmark çŠ¶æ€
                                # lin_idx = agent_idx * num_processes + proc_idx
                                current_proc_idx = lin_idx % num_processes
                                # è·å–è¯¥ç¯å¢ƒæ‰€æœ‰æ™ºèƒ½ä½“çš„ç´¢å¼•: [proc_idx, proc_idx+P, proc_idx+2P, ...]
                                process_agent_indices = torch.arange(current_proc_idx, new_detected.shape[0], num_processes)
                                new_detected[process_agent_indices, min_idx, 3] = 1.0
                
                # æ›´æ–°æ‰€æœ‰æ™ºèƒ½ä½“çš„ landmark æ•°æ®
                all_landmark_datas = new_detected
                all_landmark_masks = new_detected_masks

            # â­ å‡†å¤‡rolloutæ•°æ®
            # è½¬æ¢ ego_nodes ä¸º agent-major é¡ºåºï¼Œä»¥åŒ¹é…åç»­çš„ chunk æ“ä½œ
            # [P, A, D] -> [A, P, D] -> [A*P, D]
            all_ego_nodes = ego_nodes.transpose(0, 1).contiguous().view(num_processes * num_agents, -1)
            K = self.top_k
            # â­ ä½¿ç”¨ repeat ç¡®ä¿ç¯å¢ƒç´¢å¼•åœ¨ chunk åèƒ½æ­£ç¡®åˆ†é…ç»™æ¯ä¸ªæ™ºèƒ½ä½“
            all_teammate_nodes = teammate_nodes.repeat(num_agents, 1, 1)    # [num_agents * num_processes, num_agents, 5]
            all_teammate_masks = global_teammate_mask.repeat(num_agents, 1, 1)  # [num_agents * num_processes, num_agents, 1]
            all_explore_nodes = torch.zeros(num_processes * num_agents, K, 4, device=self.device)
            if goal_done_mask.any():
                # å°†å†³ç­–æ™ºèƒ½ä½“çš„ explore nodes å¡«å……åˆ°å¯¹åº”ä½ç½®
                all_explore_nodes[linear_indices] = batch_explore_nodes  # batch_explore_nodes: [N, K, 4]
                # å°†teammate_masksä¸­self-maskingçš„éƒ¨åˆ†ä¹Ÿæ›´æ–°
                all_teammate_masks[linear_indices] = batch_teammate_masks
                
            # 8. è®¡ç®—é«˜å±‚value
            # Vector Input: Tensor shapeÂ [Batch, N_agents, 4] [x,y,x_g,y_g]
            # Map Input: Tensor shapeÂ [Batch, 3, H, W]ã€‚
            # all_critic_vec_inp = [x,y,x_g,y_g]
            ## æ‹¼æ¥æˆ[num_processes, num_agents, 4]
            all_critic_nodes = torch.cat([agent_positions, all_goals.view(num_agents, num_processes, 2).transpose(0, 1)], dim=-1)  # [num_processes, num_agents, 4]
            all_high_value = policy.get_high_value(all_critic_map_inp, all_critic_nodes) # è®¡ç®—æ‰€æœ‰processçš„é«˜å±‚valueï¼š [num_processes, num_agents]

            # â­ åº•å±‚ç­–ç•¥
            props = policy.low_level_act(all_obs, all_goals, deterministic=False)

            # â­ æ‹†åˆ†å’Œå­˜å‚¨ç»“æœ
            n = len(team)
            
            # æ‹†åˆ†å›¾æ•°æ®
            all_ego_nodes_split = torch.chunk(all_ego_nodes, n)
            all_explore_nodes_split = torch.chunk(all_explore_nodes, n)
            all_teammate_nodes_split = torch.chunk(all_teammate_nodes, n)
            all_teammate_masks_split = torch.chunk(all_teammate_masks, n)
            all_landmark_nodes_split = torch.chunk(all_landmark_nodes, n)
            
            # æ‹†åˆ†å…¶ä»–æ•°æ®
            all_goals = torch.chunk(all_goals, n)
            all_tasks = torch.chunk(all_tasks, n)
            all_higoal_log_probs = torch.chunk(all_higoal_log_probs, n)
            all_landmark_datas_split = torch.chunk(all_landmark_datas, n)
            all_landmark_masks_split_tensor = torch.chunk(all_landmark_masks, n)
            all_high_value = torch.chunk(all_high_value, n, dim=1)
            all_value, all_action, all_action_log_prob = [torch.chunk(x, n) for x in props]
        
            for i in range(n):
                # ä½å±‚ç­–ç•¥
                team[i].value = all_value[i]
                team[i].action = all_action[i]
                team[i].action_log_prob = all_action_log_prob[i]

                # é«˜å±‚ç­–ç•¥ - åŸºç¡€æ•°æ®
                team[i].critic_map = all_critic_map_inp  # [num_processes, 3, H, W], æ™ºèƒ½ä½“å…±äº«ä¸€ä¸ªå…¨å±€å›¾
                team[i].critic_nodes = all_critic_nodes  # [num_processes, num_agents, 4]
                team[i].goal = all_goals[i]
                team[i].task = all_tasks[i]
                team[i].higoal_log_prob = all_higoal_log_probs[i]
                team[i].high_value = all_high_value[i]  # [num_processes, 1]
                
                # é«˜å±‚ç­–ç•¥ - å›¾ç»“æ„æ•°æ®ï¼ˆç”¨äºPPOæ›´æ–°ï¼‰
                team[i].ego_nodes = all_ego_nodes_split[i]  # [num_processes, 5]
                team[i].explore_nodes = all_explore_nodes_split[i]  # [num_processes, K, 4]
                team[i].landmark_data = all_landmark_datas_split[i]  # [num_processes, max_landmarks, 4]
                team[i].landmark_mask = all_landmark_masks_split_tensor[i]  # [num_processes, max_landmarks, 1]
                team[i].teammate_nodes = all_teammate_nodes_split[i]
                team[i].teammate_masks = all_teammate_masks_split[i]
                team[i].landmark_nodes = all_landmark_nodes_split[i]

                actions_list.append(all_action[i].cpu().numpy())
                goals_list.append(all_goals[i].cpu().numpy())
                tasks_list.append(all_tasks[i].cpu().numpy())

        return actions_list, goals_list, tasks_list

    def update(self):
        return_high_vals = []
        return_vals = []
        # use SMDP_ppo for training high level layer
        for i, trainer in enumerate(self.high_trainers_list):
            rollouts_list = [agent.rollouts for agent in self.teams_list[i]]
            high_vals = trainer.update(rollouts_list)
            return_high_vals.append([np.array(high_vals)]*len(rollouts_list))

        # use ippo ppo for training low level layer
        if self.use_pretrained_low_level:
            return_vals = [[np.array([0.0,0.0,0.0])]*len(self.teams_list[i]) for i in range(len(self.teams_list))]
        else:
            for i, trainer in enumerate(self.trainers_list):
                rollouts_list = [agent.rollouts for agent in self.teams_list[i]]
                vals = trainer.update(rollouts_list)
                return_vals.append([np.array(vals)]*len(rollouts_list))

        low_arr = np.stack([x for v in return_vals for x in v])     # [num_agents, 3]
        high_arr = np.stack([x for v in return_high_vals for x in v])   # [num_agents, 3]

        # === æ‹¼æ¥ === [num_agents, 8]
        return np.concatenate([low_arr, high_arr], axis=1)
    
    def update_landmark_info(self,prev_landmark_data, prev_landmark_mask, detected_map_list, device, env_dones = None, match_threshold=0.1, cleanup_threshold=0.06):
        """
        æ›´æ–°åœ°æ ‡ä¿¡æ¯ï¼Œç»“åˆä¹‹å‰çš„åœ°æ ‡æ•°æ®å’Œå½“å‰æ£€æµ‹åˆ°çš„åœ°å›¾ä¿¡æ¯ã€‚
        â­ æ–°å¢ï¼šè‡ªåŠ¨æ¸…ç†ä¸å½“å‰detected_mapä¸ä¸€è‡´çš„æ—§landmark
        
        å‚æ•°:
        - prev_landmark_data: ä¸Šä¸€æ­¥çš„åœ°æ ‡æ•°æ®ï¼Œå½¢çŠ¶ä¸º [num_agents * num_processes, max_landmarks, 4]
          æ¯ä¸ª landmark åŒ…å«: [x, y, utility, is_targeted]
        - prev_landmark_mask: ä¸Šä¸€æ­¥çš„åœ°æ ‡æ©ç ï¼Œå½¢çŠ¶ä¸º [num_agents * num_processes, max_landmarks, 1]
          1 è¡¨ç¤ºæœ‰æ•ˆï¼Œ0 è¡¨ç¤ºç©ºæ§½ä½
        - detected_map_list: list of torch.Tensor, é•¿åº¦ä¸º num_processes
          æ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [num_detected, 2]ï¼Œnum_detected å¯èƒ½ä¸åŒ
        - device: è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰
        - match_threshold: landmark åŒ¹é…çš„è·ç¦»é˜ˆå€¼
        - cleanup_threshold: æ¸…ç†æ—§landmarkçš„è·ç¦»é˜ˆå€¼ï¼ˆå¦‚æœlandmarkä¸æ‰€æœ‰detectedç‚¹çš„è·ç¦»éƒ½è¶…è¿‡æ­¤å€¼ï¼Œåˆ™ç§»é™¤ï¼‰
        - env_dones: Tensor shape [num_processes]ï¼Œè¡¨ç¤ºå“ªäº›processçš„episodeç»“æŸ
        
        è¿”å›:
        - updated_landmark_data: æ›´æ–°åçš„åœ°æ ‡æ•°æ®ï¼Œå½¢çŠ¶åŒ prev_landmark_data[num_agents * num_processes, max_landmarks, 4]
        - updated_landmark_mask: æ›´æ–°åçš„åœ°æ ‡æ©ç ï¼Œå½¢çŠ¶åŒ prev_landmark_mask[num_agents * num_processes, max_landmarks, 1]
        """
        # 1. æ·±æ‹·è´ä¸Šä¸€æ­¥çš„æ•°æ®
        updated_landmark_data = prev_landmark_data.clone()
        updated_landmark_mask = prev_landmark_mask.clone()
        
        # 2. è§£æå½¢çŠ¶ä¿¡æ¯
        num_agents_processes, max_landmarks, _ = prev_landmark_data.shape
        num_processes = len(detected_map_list)
        num_agents = num_agents_processes // num_processes

        # 3. å¤„ç† episode ç»“æŸçš„ processï¼Œæ¸…ç©ºå…¶ landmark æ•°æ®
        if env_dones is not None and env_dones.any():
            # ä½¿ç”¨å¹¿æ’­æœºåˆ¶ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å®Œæˆçš„ episodes
            # env_dones: [num_processes]ï¼ŒTrue è¡¨ç¤ºè¯¥ process çš„ episode ç»“æŸ
            
            # ç”Ÿæˆéœ€è¦æ¸…ç©ºçš„ç´¢å¼•ï¼šå¯¹äº done çš„ processï¼Œæ‰€æœ‰ agent éƒ½éœ€è¦æ¸…ç©º
            # ä½¿ç”¨ meshgrid åˆ›å»ºç´¢å¼•
            proc_indices = torch.arange(num_processes, device=self.device)[env_dones]
            agent_indices = torch.arange(num_agents, device=self.device)
            
            # ç”Ÿæˆç¬›å¡å°”ç§¯ç´¢å¼• [num_done_procs * num_agents]
            proc_mesh, agent_mesh = torch.meshgrid(proc_indices, agent_indices, indexing='ij')
            linear_indices = agent_mesh.flatten() * num_processes + proc_mesh.flatten()
            
            # ä¸€æ¬¡æ€§æ¸…ç©ºæ‰€æœ‰éœ€è¦é‡ç½®çš„ landmark æ•°æ®
            updated_landmark_mask[linear_indices, :, 0] = 0.0
            updated_landmark_data[linear_indices, :, :] = 0.0
        
        
        # 4. éå†æ¯ä¸ª processï¼Œæ›´æ–°å¯¹åº”çš„ landmark æ•°æ®
        for proc_idx, detected_map in enumerate(detected_map_list):
            # detected_map: [num_detected, 2]ï¼Œå¯èƒ½ä¸ºç©º [0, 2]
            num_detected = detected_map.shape[0]
            
            # çº¿æ€§ç´¢å¼• = agent_idx * num_processes + proc_idx
            # å¯¹äºç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“ï¼ˆagent_idx=0ï¼‰ï¼Œçº¿æ€§ç´¢å¼• = proc_idx
            linear_idx = proc_idx  # å‡è®¾å›¢é˜Ÿå…±äº«ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“çš„ç´¢å¼•
            
            # è·å–è¯¥æ™ºèƒ½ä½“å½“å‰çš„ landmarks
            current_landmarks = updated_landmark_data[linear_idx]  # [max_landmarks, 4]
            current_mask = updated_landmark_mask[linear_idx]  # [max_landmarks, 1]
            
            # ========== æ¸…ç†é€»è¾‘ ==========
            # å¦‚æœdetected_mapä¸ºç©ºï¼ˆä¾‹å¦‚ç¯å¢ƒresetåï¼‰ï¼Œæ¸…ç©ºæ‰€æœ‰landmark
            if num_detected == 0:
                updated_landmark_mask[linear_idx, :, 0] = 0.0
                updated_landmark_data[linear_idx, :, :] = 0.0  # is_targetedä¹Ÿæ¸…0
                continue
            
            # æ ‡è®°å“ªäº›æ—§landmarkéœ€è¦ä¿ç•™ï¼ˆå³ä¸detected_mapä¸­æŸä¸ªç‚¹è·ç¦»<cleanup_thresholdï¼‰
            valid_landmarks_indices = []
            for lm_idx in range(max_landmarks):
                if current_mask[lm_idx, 0] < 0.5:
                    continue  # è¯¥æ§½ä½æœ¬èº«å°±æ˜¯ç©ºçš„ï¼Œè·³è¿‡
                
                lm_pos = current_landmarks[lm_idx, :2]  # [2]
                # è®¡ç®—åˆ°æ‰€æœ‰detectedç‚¹çš„è·ç¦»
                distances_to_detected = torch.norm(detected_map - lm_pos.unsqueeze(0), dim=1)  # [num_detected]
                min_dist = distances_to_detected.min()
                
                # å¦‚æœæœ€è¿‘è·ç¦»è¶…è¿‡cleanup_thresholdï¼Œè¯´æ˜è¿™ä¸ªlandmarkå·²ç»ä¸å­˜åœ¨äº†ï¼Œç§»é™¤
                if min_dist > cleanup_threshold:
                    updated_landmark_mask[linear_idx, lm_idx, 0] = 0.0
                    updated_landmark_data[linear_idx, lm_idx, 3] = 0.0
                else:
                    valid_landmarks_indices.append(lm_idx)
            
            # ========== åŒ¹é…æˆ–æ–°å¢ landmark ==========
            for det_pos in detected_map:  # det_pos: [2]
                # 6.1 æŸ¥æ‰¾æ˜¯å¦åŒ¹é…ç°æœ‰ landmark
                matched_idx = self._find_landmark_match(det_pos, current_landmarks, 
                    current_mask, match_threshold)
                
                if matched_idx is not None:
                    # 6.2 æ›´æ–°å·²å­˜åœ¨çš„ landmarkä½ç½®ä¸ºæ–°æ—§ä½ç½®çš„åŠ æƒå¹³å‡ï¼ˆæ›´å€¾å‘æ–°ä½ç½®ï¼‰
                    old_pos = updated_landmark_data[linear_idx, matched_idx, 0:2]
                    updated_landmark_data[linear_idx, matched_idx, 0:2] = 0.3 * old_pos + 0.7 * det_pos  # 70%æ–°ä½ç½®
                    # utility å’Œ is_targeted ä¿æŒä¸å˜ï¼Œåç»­ç»Ÿä¸€æ›´æ–°
                else:
                    # 6.3 å¯»æ‰¾ç©ºé—²æ§½ä½æ·»åŠ æ–° landmark
                    empty_idx = self._find_empty_slot(updated_landmark_mask[linear_idx])
                    
                    if empty_idx is not None:
                        updated_landmark_data[linear_idx, empty_idx, 0:2] = det_pos  # x, y
                        updated_landmark_data[linear_idx, empty_idx, 2] = 2.0  # utility
                        updated_landmark_data[linear_idx, empty_idx, 3] = 0.0  # is_targeted
                        updated_landmark_mask[linear_idx, empty_idx, 0] = 1.0  # æ¿€æ´»è¯¥æ§½ä½
                    else:
                        # æ²¡æœ‰ç©ºé—²æ§½ä½ï¼Œè·³è¿‡ï¼ˆå¯ä»¥æ‰“å°è­¦å‘Šï¼‰
                        print(f"Warning: No empty slot for new landmark at process {proc_idx}")

            # ==================== 7. å¹¿æ’­ç»™è¯¥ process çš„æ‰€æœ‰æ™ºèƒ½ä½“ ====================
            # å°†æ›´æ–°åçš„ landmark æ•°æ®å¤åˆ¶ç»™è¯¥ process çš„å…¶ä»–æ™ºèƒ½ä½“
            for agent_idx in range(1, num_agents):
                broadcast_linear_idx = agent_idx * num_processes + proc_idx
                updated_landmark_data[broadcast_linear_idx] = updated_landmark_data[linear_idx].clone()
                updated_landmark_mask[broadcast_linear_idx] = updated_landmark_mask[linear_idx].clone()
        
        return updated_landmark_data.to(device), updated_landmark_mask.to(device)

    def _find_landmark_match(self, position, landmarks_data, landmarks_mask, threshold):
        """
        åœ¨å¼ é‡ä¸­æŸ¥æ‰¾åŒ¹é…çš„ landmark
        
        å‚æ•°:
        - position: [2] æ£€æµ‹åˆ°çš„ä½ç½®
        - landmarks_data: [max_landmarks, 4] landmark æ•°æ®
        - landmarks_mask: [max_landmarks, 1] æœ‰æ•ˆæ€§æ©ç 
        - threshold: åŒ¹é…é˜ˆå€¼
        
        è¿”å›:
        - matched_idx: int or None
        """
        # åªè€ƒè™‘æœ‰æ•ˆçš„ landmarks
        valid_mask = landmarks_mask[:, 0] > 0.5  # [max_landmarks]
        
        if not valid_mask.any():
            return None
        
        # è®¡ç®—è·ç¦»
        landmark_positions = landmarks_data[:, 0:2]  # [max_landmarks, 2]
        distances = torch.norm(landmark_positions - position.unsqueeze(0), dim=1)  # [max_landmarks]
        
        # å¯¹æ— æ•ˆçš„ landmark è®¾ç½®ä¸ºæ— ç©·å¤§
        distances = distances.masked_fill(~valid_mask, float('inf'))
        
        # æ‰¾åˆ°æœ€è¿‘çš„ landmark
        min_dist, min_idx = distances.min(dim=0)
        
        if min_dist < threshold:
            return min_idx.item()
        
        return None

    def _find_empty_slot(self, landmarks_mask):
        """
        æŸ¥æ‰¾ç¬¬ä¸€ä¸ªç©ºé—²æ§½ä½
        
        å‚æ•°:
        - landmarks_mask: [max_landmarks, 1] æœ‰æ•ˆæ€§æ©ç 
        
        è¿”å›:
        - empty_idx: int or None
        """
        empty_mask = landmarks_mask[:, 0] < 0.5  # [max_landmarks]
        
        if empty_mask.any():
            return empty_mask.nonzero(as_tuple=False)[0].item()
        
        return None
    
    def wrap_horizon(self):
        # éœ€è¦æ ¹æ®æœ€åä¸€æ­¥çš„obsè®¡ç®—next_valueï¼Œç„¶åä¼ å…¥æ¯ä¸ªagentçš„rolloutä¸­ï¼Œå› ä¸ºç›®çš„æ˜¯è®¡ç®—GAEï¼Œè€ŒGAEçš„æ¯ä¸€æ­¥returnéƒ½éœ€è¦ç”¨åˆ°ä¸‹ä¸€æ­¥çš„value
        # å› æ­¤éœ€è¦è®¡ç®—128æ­¥çš„goalsï¼Œä¹Ÿåªéœ€è¦æ–°goals
        for team, policy in zip(self.teams_list,self.policies_list):
            last_obs = torch.cat([agent.rollouts.obs[-1] for agent in team])
            last_masks = torch.cat([agent.rollouts.masks[-1] for agent in team])
            last_env_state = torch.cat([agent.rollouts.env_states[-1] for agent in team])

            # é»˜è®¤é‡‡å–ä¹‹å‰çš„ç›®æ ‡åˆ†é…ï¼Œ[num_agents * num_processes, 2]
            last_goals = torch.cat([agent.rollouts.goals[-1] for agent in team])

            # æå–ä¸Šä¸€æ­¥çš„ å‘ç°landamrk æ•°æ® å’Œ æ©ç 
            all_landmark_datas = torch.cat([agent.rollouts.landmark_datas[-1] for agent in team])
            all_landmark_masks = torch.cat([agent.rollouts.landmark_masks[-1] for agent in team])

            # å‘é‡åŒ–ä¼˜åŒ–=================
            # è¾“å…¥æå–
            ## æ‰¹é‡å¤„ç†æ‰€æœ‰ç¯å¢ƒ
            num_processes = len(self.envs_info)
            num_agents = len(team)

            # ä»all_masksä¸­æå–env_doneä¿¡æ¯ï¼Œå…¨0è¡¨ç¤ºå½“å‰processçš„episodeç»“æŸ
            episode_dones = last_masks.view(num_agents, num_processes).transpose(0, 1)
            env_dones = (episode_dones.sum(dim=1) == 0)  # [num_processes]ï¼ŒTrueè¡¨ç¤ºè¯¥processçš„episodeç»“æŸ

            # 1. æ‰¹é‡æ„å»º critic map input å’Œ critic vec input
            entropy_maps = torch.stack([torch.from_numpy(np.array(info['entropy_map'])).float() 
                                        for info in self.envs_info]).to(self.device)  # [num_processes, H, W]
            heatmaps = torch.stack([torch.from_numpy(np.array(info['heatmap'])).float() 
                                    for info in self.envs_info]).to(self.device)  # [num_processes, H, W]
            landmark_heatmaps = torch.stack([torch.from_numpy(np.array(info['landmark_heatmap'])).float() 
                                            for info in self.envs_info]).to(self.device)  # [num_processes, H, W]

            all_critic_map_inp = torch.stack([entropy_maps, heatmaps, landmark_heatmaps], dim=1)  # [num_processes, 3, H, W]

            # 2. æ”¶é›†æ‰€æœ‰ goal_done çŠ¶æ€å¹¶æ„å»ºmask
            goal_done_list = [info['goal_done'] for info in self.envs_info]  # list of lists
            goal_done_mask = torch.tensor(goal_done_list, dtype=torch.bool, device=self.device)  # [num_processes, num_agents]
            # æ”¶é›†å½“å‰çš„ world_step ä¿¡æ¯ï¼Œå¹¶å¹¿æ’­ä¸º [num_processes, num_agents, 1] çš„å¼ é‡
            agent_world_steps = torch.tensor(
                [info['world_steps'] for info in self.envs_info], 
                dtype=torch.float32, 
                device=self.device
            ).unsqueeze(1).repeat(1, num_agents).unsqueeze(-1)  # [num_processes, num_agents, 1]
            # æ ¹æ® world_step ç”Ÿæˆ battery ä¿¡æ¯: battery = (50 - step) / 50
            agent_batterys = (50.0 - agent_world_steps) / 50.0  # [num_processes, num_agents, 1]

            # 3. æ‰¹é‡ç”Ÿæˆ landmark node
            detected_maps = [torch.from_numpy(np.array(info['map'][1])).float().to(self.device) 
                            for info in self.envs_info] # [num_processes, x, 2], ä¸è§„åˆ™å½¢çŠ¶ï¼Œæ¯ä¸ª process æ£€æµ‹åˆ°çš„ç›®æ ‡æ•°é‡ä¸åŒ
            
            new_detected, new_detected_masks = self.update_landmark_info(all_landmark_datas, all_landmark_masks, detected_maps, self.device, env_dones) 
            # [num_agents * num_processes, max_landmarks, 4], [num_agents * num_processes, max_landmarks, 1]

            # 4. æ‰¹é‡é€šè¿‡RTTç”ŸæˆKä¸ªå€™é€‰ç›®æ ‡ç‚¹
            ## ç”Ÿæˆä¸¤å¼ åœ°å›¾
            agent_entropy_maps = entropy_maps.unsqueeze(1).repeat(1, num_agents, 1, 1)  # [num_processes, num_agents, H, W]
            voronoi_masks = torch.stack([
                torch.stack([torch.from_numpy(np.array(info['voronoi_masks'][a])).float() 
                            for a in range(num_agents)]) 
                for info in self.envs_info
            ]).to(self.device)  # [num_processes, num_agents, H, W]   

            ## ä»all_obsæ‰¹é‡ç”Ÿæˆæ™ºèƒ½ä½“ä¿¡æ¯ï¼ˆæ …æ ¼ç´¢å¼•ï¼‰ [num_processes, num_agents, 2]
            agent_positions = last_obs[:, 2:4].view(num_agents, num_processes, 2).transpose(0, 1) # ä½ç½®[num_processes, num_agents, 2]
            agent_vels = last_obs[:, 0:2].view(num_agents, num_processes, 2).transpose(0, 1)      # é€Ÿåº¦[num_processes, num_agents, 2]
            agent_goals = last_goals.view(num_agents, num_processes, 2).transpose(0, 1) # [num_processes, num_agents, 2]
            ## æ‹¼æ¥æˆ[num_processes, num_agents, 4]
            agent_nodes = torch.cat([agent_positions, agent_goals], dim=-1)  # [num_processes, num_agents, 4]
            ego_nodes = torch.cat([agent_positions, agent_vels, agent_batterys], dim=-1)  # [num_processes, num_agents, 5]
            
            # â­ å‡†å¤‡ Teammate Nodes and Masks
            dist_to_goal = torch.norm(agent_goals - agent_positions, dim=-1, keepdim=True)
            teammate_nodes = torch.cat([agent_positions, agent_vels, dist_to_goal], dim=-1) # [P, A, 5]
            
            # Global mask from last_masks
            global_teammate_mask = last_masks.view(num_agents, num_processes).t().unsqueeze(-1) # [P, A, 1]

            if goal_done_mask.any():
                # è·å–éœ€è¦æ›´æ–°çš„ç´¢å¼• (process_idx, agent_idx)
                update_indices = torch.nonzero(goal_done_mask, as_tuple=False)  # [N, 2] where N is number of True values
                
                # é€‰æ‹©éœ€è¦æ›´æ–°çš„æ™ºèƒ½ä½“æ‰€åœ¨ç¯å¢ƒè¾“å…¥
                proc_indices = update_indices[:, 0]
                agent_indices = update_indices[:, 1]
                
                map_inps = torch.stack([
                    agent_entropy_maps[proc_indices],
                    voronoi_masks[proc_indices]
                ], dim=0)  # [2, N, num_agents, H, W]
                
                vec_inp_agents = agent_nodes[proc_indices]  # [N, num_agents, 4]

                # â­ ç”Ÿæˆæœ¬æ¬¡å†³ç­–æ‰€éœ€çš„ batch_teammate_masks [N, A, 1]
                batch_teammate_nodes = teammate_nodes[proc_indices] # [N, A, 5]
                # 1. ä»å…¨å±€æ©ç ä¸­æå–å¯¹åº”ç¯å¢ƒçš„æ©ç 
                batch_teammate_masks = global_teammate_mask[proc_indices].clone() # [N, A, 1]
                # 2. æ’é™¤è‡ªå·± (self-masking)
                batch_indices = torch.arange(len(proc_indices), device=self.device)
                batch_teammate_masks[batch_indices, agent_indices, 0] = 0.0

                # 4.2 é€šè¿‡RTTç”Ÿæˆå€™é€‰æ¢ç´¢ç‚¹
                batch_explore_nodes = policy.get_explore_nodes(self.top_k, self.rrt_max_iter, vec_inp_agents, map_inps, agent_indices)  # [B_pro, B_agents, K, 4]
                batch_explore_nodes = batch_explore_nodes.reshape(-1, batch_explore_nodes.shape[-2], batch_explore_nodes.shape[-1])  # [B_pro*B_agents, K, 4]
                # 4.3 ego nodes
                batch_ego_nodes = ego_nodes[proc_indices, agent_indices]  # [N, 5]
                # 4.4 landmark nodes
                linear_indices = agent_indices * num_processes + proc_indices
                batch_landmark_nodes, batch_landmark_node_masks = policy.get_landmark_nodes(
                    last_obs[:,2:4],  # [num_agents * num_processes, 2]
                    new_detected,                   # [num_agents * num_processes, max_landmarks, 4]
                    new_detected_masks,             # [num_agents * num_processes, max_landmarks, 1]
                    linear_indices                  # [N]
                )  # List of [L_i, 4], List of [L_i, 1]

                # 4.5 edge features
                batch_ego_to_explore_edges, batch_ego_to_landmark_edges, batch_ego_to_landmark_edge_masks = policy.get_edge_features(
                    batch_explore_nodes,    # batch_ego_to_explore_edges: List of [K, 3], é•¿åº¦ä¸º N
                    batch_landmark_nodes,   # batch_ego_to_landmark_edges: List of [L_i, 3], é•¿åº¦ä¸º N
                    batch_landmark_node_masks   # batch_ego_to_landmark_edge_masks: List of [L_i, 1], é•¿åº¦ä¸º N
                )
                
                # 5. æ‰¹é‡æ‰§è¡Œé«˜å±‚ç­–ç•¥
                batch_goals = policy.get_high_level_goal(
                    batch_ego_nodes, # Tensor shapeÂ [N, 5]
                    batch_teammate_nodes,
                    batch_teammate_masks,
                    batch_explore_nodes, # Tensor shapeÂ [N, K, 4]
                    batch_ego_to_explore_edges, # TensorÂ [N, K, 3], length N
                    batch_landmark_nodes, # List of Tensor shapeÂ [L_i, 4], length N
                    batch_landmark_node_masks, # List of Tensor shapeÂ [L_i, 1], length N
                    batch_ego_to_landmark_edges, # Tensor [N, Max_L, 3]
                    batch_ego_to_landmark_edge_masks # Tensor [N, Max_L, 1]
                )

                # 6.2 æ›´æ–°ç›®æ ‡å’Œä»»åŠ¡
                last_goals[linear_indices] = batch_goals["waypoints"]  # [N, 2]
            
            all_critic_nodes = torch.cat([last_obs[:,2:4].view(num_agents, num_processes, -1).transpose(0, 1), 
                                          last_goals.view(num_agents, num_processes, 2).transpose(0, 1)], dim=-1)
            with torch.no_grad():
                next_high_value = policy.get_high_value(all_critic_map_inp, all_critic_nodes) # è®¡ç®—æ‰€æœ‰processçš„é«˜å±‚valueï¼š [num_processes, num_agents]
                next_low_value = policy.get_low_value(last_obs, last_goals)

            all_high_value = torch.chunk(next_high_value,len(team), dim=1)
            all_low_value = torch.chunk(next_low_value,len(team))
            for i in range(len(team)):
                team[i].wrap_horizon(all_low_value[i], all_high_value[i])

    def after_update(self):
        for agent in self.all_agents:
            agent.after_update()
    
    def initial_hidden_states(self, step):
        for agent in self.all_agents:
            agent.initial_hidden_states(step)

    def update_rollout(self, obs, reward, high_rewards, masks, env_state, goal_dones):
        obs_t = torch.from_numpy(obs).float().to(self.device)
        env_state_t = torch.from_numpy(env_state).float().to(self.device)
        for i, agent in enumerate(self.all_agents):
            agent_obs = obs_t[:, i, :]
            agent.update_rollout(agent_obs, reward[:,i].unsqueeze(1), high_rewards[:,i].unsqueeze(1), 
                                 masks[:,i].unsqueeze(1), env_state_t, goal_dones[:,i].unsqueeze(1))

    def load_models(self, policies_list):
        for agent, policy in zip(self.all_agents, policies_list):
            agent.load_model(policy)

    def eval_act(self, obs, env_states, masks, goals, tasks, landmark_data, landmark_mask, deterministic=True):
        # used only while evaluating policies. Assuming that agents are in order of team!
        # goals: ä¸Šä¸€æ­¥çš„ç›®æ ‡åˆ†é… [num_agents, 2]
        # landmark_data: ä¸Šä¸€æ­¥çš„åœ°æ ‡æ•°æ® [num_agents, max_landmarks, 4]
        # landmark_mask: ä¸Šä¸€æ­¥çš„åœ°æ ‡æ©ç  [num_agents, max_landmarks, 1]
        obs1 = []
        obs2 = []
        all_obs = []
        for i in range(len(obs)):
            agent = self.env.world.policy_agents[i]
            if hasattr(agent, 'adversary') and agent.adversary:
                obs1.append(torch.as_tensor(obs[i],dtype=torch.float,device=self.device).view(1,-1))
            else:
                obs2.append(torch.as_tensor(obs[i],dtype=torch.float,device=self.device).view(1,-1))
        if len(obs1)!=0:
            all_obs.append(obs1)
        if len(obs2)!=0:
            all_obs.append(obs2)

        actions = []
        # è¿™é‡Œéœ€è¦å¯¹env_statesè¿›è¡Œå¤„ç†ï¼Œå› ä¸ºå®ƒæ˜¯(env_state_dim)çš„å½¢çŠ¶ï¼Œéœ€è¦å¤åˆ¶æˆ(num_agent, env_state_dim)
        env_states = torch.from_numpy(env_states).float().to(self.device)
        for team,policy,obs in zip(self.teams_list,self.policies_list,all_obs):
            # é»˜è®¤é‡‡å–ä¹‹å‰çš„ç›®æ ‡åˆ†é…ï¼Œ[num_agents, 2]
            all_goals = goals
            all_tasks = tasks

            num_agents = len(team)

            # 1. æ”¶é›†æ•°æ®

            obs_tensor = torch.cat(obs, dim=0).to(self.device) # [num_agents, obs_dim]

            # 1.1. æ„å»º entropy_map, heatmap, landmark_heatmap
            entropy_map = torch.from_numpy(np.array(self.envs_info['entropy_map'])).float().unsqueeze(0).to(self.device)  # [1, H, W]
            heatmap = torch.from_numpy(np.array(self.envs_info['heatmap'])).float().unsqueeze(0).to(self.device)  # [1, H, W]
            landmark_heatmap = torch.from_numpy(np.array(self.envs_info['landmark_heatmap'])).float().unsqueeze(0).to(self.device)  # [1, H, W]

            # 1.2. æ”¶é›†æ‰€æœ‰ goal_done çŠ¶æ€å¹¶æ„å»ºmask
            goal_done_list = [self.envs_info['goal_done']]  # âœ… åŒ…è£…æˆåˆ—è¡¨
            goal_done_mask = torch.tensor(goal_done_list, dtype=torch.bool, device=self.device)  # [1, num_agents]   

            # 1.3. æ”¶é›†å½“å‰çš„ world_step ä¿¡æ¯ï¼Œå¹¶å¹¿æ’­ä¸º [1, num_agents, 1] çš„å¼ é‡
            agent_world_steps = torch.tensor(
                [self.envs_info['world_steps']], 
                dtype=torch.float32, 
                device=self.device
            ).unsqueeze(1).repeat(1, num_agents).unsqueeze(-1)  # [1, num_agents, 1]
            # æ ¹æ® world_step ç”Ÿæˆ battery ä¿¡æ¯: battery = (50 - step) / 50
            agent_batterys = (50.0 - agent_world_steps) / 50.0  # [1, num_agents, 1]

            # 1.4. æ”¶é›† landmark data å’Œ mask
            detected_map = torch.from_numpy(np.array(self.envs_info['map'][1])).float().to(self.device)

            new_detected, new_detected_masks = self.update_landmark_info(
                landmark_data, 
                landmark_mask, 
                [detected_map], 
                self.device
            )  # [num_agents * 1, max_landmarks, 4], [num_agents * 1, max_landmarks, 1]

            # 2. ç”ŸæˆåŠ¨æ€å¼‚æ„å›¾ç»“æ„çš„èŠ‚ç‚¹è¡¨ç¤º
            # 2.1. è¾“å…¥å‡†å¤‡ï¼ŒåŒ…æ‹¬åœ°å›¾è¾“å…¥å’Œå‘é‡è¾“å…¥
            agent_entropy_map = entropy_map.unsqueeze(1).repeat(1, num_agents, 1, 1) # [num_processes, num_agents, H, W]
            voronoi_masks = torch.stack([
                torch.from_numpy(np.array(self.envs_info['voronoi_masks'][a])).float()
                for a in range(num_agents)
            ]).unsqueeze(0).to(self.device)  # [1, num_agents, H, W]

            ## ä»all_obsæ‰¹é‡ç”Ÿæˆæ™ºèƒ½ä½“ä¿¡æ¯ï¼ˆæ …æ ¼ç´¢å¼•ï¼‰ [num_processes, num_agents, 2]
            agent_positions = obs_tensor[:, 2:4].view(1, num_agents, 2) # ä½ç½®[1, num_agents, 2]
            agent_vels = obs_tensor[:, 0:2].view(1, num_agents, 2)      # é€Ÿåº¦[1, num_agents, 2]
            agent_goals = all_goals.view(1, num_agents, 2) # [1, num_agents, 2]
            ## æ‹¼æ¥æˆ[1, num_agents, 4]
            agent_nodes = torch.cat([agent_positions, agent_goals], dim=-1)  # [1, num_agents, 4]
            ego_nodes = torch.cat([agent_positions, agent_vels, agent_batterys], dim=-1)  # [1, num_agents, 5]
            
            # â­ å‡†å¤‡ Teammate Nodes and Masks
            dist_to_goal = torch.norm(agent_goals - agent_positions, dim=-1, keepdim=True)
            teammate_nodes = torch.cat([agent_positions, agent_vels, dist_to_goal], dim=-1) # [1, A, 5]
            
            # Global mask (all alive in eval)
            global_teammate_mask = masks.view(num_agents, 1).t().unsqueeze(-1) # [1, A, 1]

            if goal_done_mask.any():
                # è·å–éœ€è¦æ›´æ–°çš„ç´¢å¼• (process_idx, agent_idx)
                update_indices = torch.nonzero(goal_done_mask, as_tuple=False)  # [N, 2] where N is number of True values
                   
                # åªé€‰æ‹©éœ€è¦æ›´æ–°çš„æ™ºèƒ½ä½“
                proc_indices = update_indices[:, 0]
                agent_indices = update_indices[:, 1]
                
                map_inps = torch.stack([
                    agent_entropy_map[proc_indices],
                    voronoi_masks[proc_indices],
                ], dim=0)  # [2, N, num_agents, H, W]
                
                vec_inp_agents = agent_nodes[proc_indices]  # [N, num_agents, 4]

                # â­ ç”Ÿæˆæœ¬æ¬¡å†³ç­–æ‰€éœ€çš„ batch_teammate_masks [N, A, 1]
                batch_teammate_nodes = teammate_nodes[proc_indices] # [N, A, 5]
                # 1. ä»å…¨å±€æ©ç ä¸­æå–å¯¹åº”ç¯å¢ƒçš„æ©ç 
                batch_teammate_masks = global_teammate_mask[proc_indices].clone() # [N, A, 1]
                # 2. æ’é™¤è‡ªå·± (self-masking)
                batch_indices = torch.arange(len(proc_indices), device=self.device)
                batch_teammate_masks[batch_indices, agent_indices, 0] = 0.0

                # 2.2. é€šè¿‡RTTç”Ÿæˆå€™é€‰æ¢ç´¢ç‚¹
                batch_explore_nodes = policy.get_explore_nodes(self.top_k, self.rrt_max_iter, vec_inp_agents, map_inps, agent_indices)  # [B_pro, B_agents, K, 4]
                batch_explore_nodes = batch_explore_nodes.reshape(-1, batch_explore_nodes.shape[-2], batch_explore_nodes.shape[-1])  # [B_pro*B_agents, K, 4]
                # 2.3. ego nodes
                batch_ego_nodes = ego_nodes[proc_indices, agent_indices]  # [N, 5]
                # 2.4. landmark nodes
                linear_indices = agent_indices * 1 + proc_indices
                batch_landmark_nodes, batch_landmark_node_masks = policy.get_landmark_nodes(
                    obs_tensor[:,2:4],  # [num_agents * 1, 2]
                    new_detected,                   # [num_agents * 1, max_landmarks, 4]
                    new_detected_masks,             # [num_agents * 1, max_landmarks, 1]
                    linear_indices                  # [N]
                )  # List of [L_i, 4], List of [L_i, 1]

                # 2.5. edge features
                batch_ego_to_explore_edges, batch_ego_to_landmark_edges, batch_ego_to_landmark_edge_masks = policy.get_edge_features(
                    batch_explore_nodes,    # batch_ego_to_explore_edges: List of [K, 3], é•¿åº¦ä¸º N
                    batch_landmark_nodes,   # batch_ego_to_landmark_edges: List of [L_i, 3], é•¿åº¦ä¸º N
                    batch_landmark_node_masks   # batch_ego_to_landmark_edge_masks: List of [L_i, 1], é•¿åº¦ä¸º N
                )

                # 3. æ‰¹é‡æ‰§è¡Œé«˜å±‚ç­–ç•¥
                batch_goals = policy.get_high_level_goal(
                    batch_ego_nodes, # Tensor shapeÂ [N, 5]
                    batch_teammate_nodes,
                    batch_teammate_masks,
                    batch_explore_nodes, # Tensor shapeÂ [N, K, 4]
                    batch_ego_to_explore_edges, # TensorÂ [N, K, 3], length N
                    batch_landmark_nodes, # List of Tensor shapeÂ [L_i, 4], length N
                    batch_landmark_node_masks, # List of Tensor shapeÂ [L_i, 1], length N
                    batch_ego_to_landmark_edges, # Tensor [N, Max_L, 3]
                    batch_ego_to_landmark_edge_masks, # Tensor [N, Max_L, 1]
                    deterministic  = deterministic
                    )  # éœ€è¦å®ç°batchç‰ˆæœ¬
                
                # 4. æ‰¹é‡æ›´æ–° all_goals å’Œ all_tasks
                # 4.1.è®¡ç®—çº¿æ€§ç´¢å¼•: agent_idx * num_processes + process_idx
                linear_indices = agent_indices * 1 + proc_indices
                
                # 4.2. æ›´æ–°ç›®æ ‡å’Œä»»åŠ¡
                all_goals[linear_indices] = batch_goals["waypoints"]  # [N, 2] è½¬æ¢ä¸ºfloat
                all_tasks[linear_indices] = batch_goals["action_modes"]  # [N, 1] è½¬æ¢ä¸ºfloat

                # 4.3. æ›´æ–° landmark data å’Œ mask
                # å¦‚æœæ™ºèƒ½ä½“é€‰æ‹©çš„ä»»åŠ¡æ˜¯ landmarkï¼Œåˆ™æ›´æ–°å¯¹åº” landmark çš„ is_targeted å±æ€§
                for i, lin_idx in enumerate(linear_indices):
                    if batch_goals["action_modes"][i, 0] == 1:  # é€‰æ‹©çš„æ˜¯ landmark
                        # è·å–é€‰ä¸­çš„ waypointï¼ˆç»å¯¹ä¸–ç•Œåæ ‡ï¼‰
                        selected_waypoint = batch_goals["waypoints"][i]  # [2]
                        
                        # åœ¨å¯¹åº”çš„ landmark åˆ—è¡¨ä¸­æ‰¾åˆ°åŒ¹é…çš„ landmark
                        landmarks = new_detected[lin_idx]  # [max_landmarks, 4]
                        landmark_mask = new_detected_masks[lin_idx]  # [max_landmarks, 1]
                        
                        # æ‰¾åˆ°æœ‰æ•ˆçš„ landmark
                        valid_mask = landmark_mask[:, 0] > 0.5
                        if valid_mask.any():
                            # è®¡ç®—è·ç¦»ï¼ˆä½¿ç”¨ç»å¯¹åæ ‡ï¼‰
                            landmark_positions = landmarks[:, :2]  # [max_landmarks, 2]
                            distances = torch.norm(landmark_positions - selected_waypoint, dim=1)  # [max_landmarks]
                            distances = distances.masked_fill(~valid_mask, float('inf'))
                            
                            # æ‰¾åˆ°æœ€è¿‘çš„ landmark
                            min_idx = distances.argmin()
                            if distances[min_idx] < 0.05:  # åŒ¹é…é˜ˆå€¼ 0.05
                                # new_detected[lin_idx, min_idx, 3] = 1.0  # è®¾ç½® is_targeted = 1
                                # æ‰€æœ‰agentçš„éšœç¢ç‰©is_targetedåŒæ­¥æ›´æ–°
                                # åœ¨ eval æ¨¡å¼ä¸‹ num_processes=1ï¼Œ[:] æ˜¯å®‰å…¨çš„ï¼Œä½†ä¸ºäº†é€»è¾‘ç»Ÿä¸€ï¼š
                                new_detected[:, min_idx, 3] = 1.0
                
                # æ›´æ–°æ‰€æœ‰æ™ºèƒ½ä½“çš„ landmark_data å’Œ landmark_mask
                landmark_data = new_detected
                landmark_mask = new_detected_masks

            if len(obs)!=0:
                _,action,_ = policy.low_level_act(obs_tensor, all_goals, deterministic=True)
                actions.append(action.squeeze(1).cpu().numpy())

        return np.hstack(actions), all_goals, all_tasks, landmark_data, landmark_mask

    def eval_reward_choose(self, all_rewards, task):
        n,dim = task.shape
        task_reshape = task.transpose(0,1).reshape(n*dim)
        masked = all_rewards * task_reshape
        agent_reward = masked.view(dim, n).sum(dim=0)
        return agent_reward

    def set_eval_mode(self):
        for agent in self.all_agents:
            agent.actor_critic.eval()

    def set_train_mode(self):
        for agent in self.all_agents:
            agent.actor_critic.train()
