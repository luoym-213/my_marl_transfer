import torch
import torch.nn as nn
import numpy as np
from rlcore.distributions import Categorical
import torch.nn.functional as F
import math
from scipy.optimize import linear_sum_assignment  # åŒˆç‰™åˆ©ç®—æ³•
from torch.distributions import Categorical as TorchCategorical
from planning.rrt_GNN import RRT_GNN, plan_batch

def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1 or classname.find('Linear') != -1:
        nn.init.orthogonal_(m.weight.data)
        if m.bias is not None:
            m.bias.data.fill_(0)


class MPNN(nn.Module):
    def __init__(self, action_space, num_agents, num_entities, input_size=16, hidden_dim=128, embed_dim=None,
                 pos_index=2, norm_in=False, nonlin=nn.ReLU, n_heads=3, mask_dist=None, mask_obs_dist=None, entity_mp=False, is_recurrent=True):
        super().__init__()

        # ==================== åŸºç¡€é…ç½® ====================
        self.h_dim = hidden_dim
        self.nonlin = nonlin
        self.num_agents = num_agents # number of agents
        self.num_entities = num_entities # number of entities
        self.low_level_input = 2 + 2*num_agents # low level input size: agent pos + all agents pos
        self.K = 3 # message passing rounds
        self.embed_dim = self.h_dim if embed_dim is None else embed_dim
        self.n_heads = n_heads
        self.is_recurrent = is_recurrent
        self.mask_dist = mask_dist
        self.mask_obs_dist = mask_obs_dist
        self.input_size = input_size # è¿™é‡Œæ˜¯agengtè‡ªèº«é€Ÿåº¦ä½ç½®ï¼ˆ4ï¼‰
        self.entity_mp = entity_mp
        # this index must be from the beginning of observation vector
        self.pos_index = pos_index
        # task generation parameters
        self.task_dim = 2
        self.h_dim2 = self.h_dim // 2 # 64
        num_actions = action_space.n

        # ==================== æ¨¡å—åŒ–ç½‘ç»œ ====================
        self.modules_dict = nn.ModuleDict()
        
        # 1. åº•å±‚ç­–ç•¥ç½‘ç»œ
        self.modules_dict['low_level'] = self._build_low_level_modules(action_space)
        
        # 2. é«˜å±‚ç­–ç•¥ç½‘ç»œ
        self.modules_dict['high_level'] = self._build_high_level_modules()
        
        # 3. é«˜å±‚ Critic
        self.modules_dict['high_critic'] = self._build_high_critic_modules()

        # ==================== å…¶ä»–å±æ€§ ====================
        if norm_in:
            self.in_fn = nn.BatchNorm1d(self.input_size)
            self.in_fn.weight.data.fill_(1)
            self.in_fn.bias.data.fill_(0)
        else:
            self.in_fn = lambda x: x
        self.apply(weights_init)
        self.attn_mat = np.ones((num_agents, num_agents))
        self.dropout_mask = None     
        

        # self.value_head = nn.Sequential(nn.Linear(self.h_dim, self.h_dim),
        #                                 self.nonlin(inplace=True),
        #                                 nn.Linear(self.h_dim,1))

        # self.policy_head = nn.Sequential(nn.Linear(self.h_dim, self.h_dim),
        #                                  self.nonlin(inplace=True))

        # self.low_agent_encoder = nn.Sequential(nn.Linear(self.low_level_input, self.h_dim),
        #                                       self.nonlin(inplace=True))
        
        # ==================== ä»£åŠ ====================
        self.dist = Categorical(self.h_dim,num_actions)

    # ==================== æ¨¡å—æ„å»ºå‡½æ•° ====================
    def _build_low_level_modules(self, action_space):
        """æ„å»ºåº•å±‚ç­–ç•¥ç½‘ç»œ"""
        num_actions = action_space.n
        low_level = nn.ModuleDict({
            'encoder': nn.Sequential(
                nn.Linear(self.low_level_input, self.h_dim),
                self.nonlin(inplace=True)
            ),
            'value_head': nn.Sequential(
                nn.Linear(self.h_dim, self.h_dim),
                self.nonlin(inplace=True),
                nn.Linear(self.h_dim, 1)
            ),
            'policy_head': nn.Sequential(
                nn.Linear(self.h_dim, self.h_dim),
                self.nonlin(inplace=True)
            ),
            'dist': Categorical(self.h_dim, num_actions)
        })
        
        return low_level

    def _build_high_level_modules(self):
        """æ„å»ºé«˜å±‚ç­–ç•¥ç½‘ç»œï¼ˆActorï¼‰"""
        high_level = nn.ModuleDict({
            # åœ°å›¾ç¼–ç å™¨
            'map_conv1': nn.Sequential(
                nn.Conv2d(4, 16, kernel_size=5, stride=2, padding=2),
                nn.ReLU()
            ),
            'map_conv2': nn.Sequential(
                nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),
                nn.ReLU()
            ),
            'map_conv3': nn.Sequential(
                nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=0),
                nn.ReLU()
            ),
            
            # å‘é‡ç¼–ç å™¨
            'vec_mlp': nn.Sequential(
                nn.Linear(5, 32),
                nn.ReLU(),
                nn.Linear(32, 64),
                nn.ReLU()
            ),
            
            # ===== åŠ¨æ€å›¾æ¨ç†æ¨¡å— =====
            # EgoèŠ‚ç‚¹ç¼–ç å™¨ [5] -> [64]
            'ego_node_encoder': nn.Sequential(
                nn.Linear(5, 32),
                nn.ReLU(),
                nn.Linear(32, 64),
                nn.ReLU()
            ),

            # teammateèŠ‚ç‚¹ç¼–ç å™¨ [5] -> [64]
            'teammate_node_encoder': nn.Sequential(
                nn.Linear(5, 32),
                nn.ReLU(),
                nn.Linear(32, 64),
                nn.ReLU()
            ),
            
            # ExploreèŠ‚ç‚¹ç¼–ç å™¨ [4] -> [64]
            'explore_node_encoder': nn.Sequential(
                nn.Linear(4, 32),
                nn.ReLU(),
                nn.Linear(32, 64),
                nn.ReLU()
            ),
            
            # LandmarkèŠ‚ç‚¹ç¼–ç å™¨ [4] -> [64]
            'landmark_node_encoder': nn.Sequential(
                nn.Linear(4, 32),
                nn.ReLU(),
                nn.Linear(32, 64),
                nn.ReLU()
            ),

            # exploreçº¿æ€§æ˜ å°„å±‚ [64] -> [64]
            'explore_node_linear': nn.Linear(64, 64),

            # landmarkçº¿æ€§æ˜ å°„å±‚ [64] -> [64]
            'landmark_node_linear': nn.Linear(64, 64),

            'linear_ln': nn.LayerNorm(64),
            
            # è¾¹ç¼–ç å™¨ [3] -> [32]
            'edge_encoder': nn.Sequential(
                nn.Linear(3, 32),
                nn.ReLU()
            ),
            
            # æ³¨æ„åŠ›æŠ•å½±å±‚
            'q_proj': nn.Linear(64, 64),  # ego -> query
            'k_proj': nn.Linear(96, 64),  # node(64) + edge(32) -> key
            'v_proj': nn.Linear(96, 64),  # node(64) + edge(32) -> value
            
            # èŠ‚ç‚¹é€‰æ‹©å¤´ï¼ˆç»Ÿä¸€å¯¹æ‰€æœ‰èŠ‚ç‚¹æ‰“åˆ†ï¼‰
            'node_selection_head': nn.Linear(64, 1),
            
            # å†³ç­–å¤´ï¼ˆä¿ç•™ç”¨äºå…¶ä»–ç”¨é€”ï¼Œå¦‚æœä¸éœ€è¦å¯ä»¥åˆ é™¤ï¼‰
            'decision_head': nn.Sequential(
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Linear(64, 2)
            ),
            
            # æ¢ç´¢ç‚¹è§£ç å™¨
            'decoder_fuse': nn.Sequential(
                nn.Conv2d(192, 64, kernel_size=1),
                nn.ReLU()
            ),
            'decoder_up1': nn.Sequential(
                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
                nn.Conv2d(64, 32, kernel_size=3, padding=1),
                nn.ReLU()
            ),
            'decoder_up2': nn.Sequential(
                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
                nn.Conv2d(32, 16, kernel_size=3, padding=1),
                nn.ReLU()
            ),
            'decoder_out': nn.Sequential(
                nn.Upsample(size=(100, 100), mode='bilinear', align_corners=False),
                nn.Conv2d(16, 1, kernel_size=1)
            )
        })
        
        return high_level

    def _build_high_critic_modules(self):
        """æ„å»ºé«˜å±‚ Critic"""
        high_critic = nn.ModuleDict({
            # å…¨å±€åœ°å›¾ç¼–ç å™¨ [B, 3, H, W] -> [B, 256]
            'map_backbone': nn.Sequential(
                nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=2),
                nn.ReLU(),
                nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),
                nn.ReLU(),
                nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=0),
                nn.ReLU()
            ),
            'map_compress': nn.Sequential(
                nn.Linear(64 * 6 * 6, 256),
                nn.ReLU()
            ),
            
            # å•ä¸ªæ™ºèƒ½ä½“çŠ¶æ€ç¼–ç å™¨ [4] -> [64]
            'agent_encoder': nn.Sequential(
                nn.Linear(4, 32),  # [x, y, x_g, y_g]
                nn.ReLU(),
                nn.Linear(32, 64),
                nn.ReLU()
            ),
            
            # èåˆå±‚ï¼šå…¨å±€ç‰¹å¾(256) + æ™ºèƒ½ä½“ç‰¹å¾(64) -> ä»·å€¼
            'fusion_layer': nn.Sequential(
                nn.Linear(256 + 64, 128),
                nn.ReLU(),
                nn.Linear(128, 128),
                nn.ReLU()
            )
        })
        
        # æ¯ä¸ªæ™ºèƒ½ä½“çš„ç‹¬ç«‹ä»·å€¼è¾“å‡ºå¤´
        high_critic['value_heads'] = nn.ModuleList([
            nn.Linear(128, 1) for _ in range(self.num_agents)
        ])
        
        return high_critic

    # ==================== å‚æ•°ç®¡ç†æ¥å£ ====================
    
    def get_module_params(self, module_name):
        """
        è·å–æŒ‡å®šæ¨¡å—çš„å‚æ•°
        
        Args:
            module_name: 'shared', 'low_level', 'high_level', 'high_critic'
        
        Returns:
            list of parameters
        """
        if module_name not in self.modules_dict:
            raise ValueError(f"Module '{module_name}' not found! Available: {list(self.modules_dict.keys())}")
        
        return list(self.modules_dict[module_name].parameters())

    def freeze_module(self, module_name):
        """å†»ç»“æŒ‡å®šæ¨¡å—çš„å‚æ•°"""
        for param in self.get_module_params(module_name):
            param.requires_grad = False
        print(f"âœ… Module '{module_name}' frozen")

    def unfreeze_module(self, module_name):
        """è§£å†»æŒ‡å®šæ¨¡å—çš„å‚æ•°"""
        for param in self.get_module_params(module_name):
            param.requires_grad = True
        print(f"âœ… Module '{module_name}' unfrozen")

    def save_module_checkpoint(self, module_name, path):
        """
        ä¿å­˜æŒ‡å®šæ¨¡å—çš„å‚æ•°
        
        Args:
            module_name: æ¨¡å—åç§°
            path: ä¿å­˜è·¯å¾„
        """
        if module_name not in self.modules_dict:
            raise ValueError(f"Module '{module_name}' not found!")
        
        checkpoint = {
            'module_name': module_name,
            'state_dict': self.modules_dict[module_name].state_dict(),
            'config': {
                'num_agents': self.num_agents,
                'num_entities': self.num_entities,
                'hidden_dim': self.h_dim,
                'embed_dim': self.embed_dim
            }
        }
        
        torch.save(checkpoint, path)
        print(f"âœ… Saved '{module_name}' checkpoint to {path}")

    def load_module_checkpoint(self, module_name, path, strict=True, freeze=False):
        """
        åŠ è½½æŒ‡å®šæ¨¡å—çš„å‚æ•°
        
        Args:
            module_name: æ¨¡å—åç§°
            path: checkpoint è·¯å¾„
            strict: æ˜¯å¦ä¸¥æ ¼åŒ¹é…å‚æ•°
            freeze: æ˜¯å¦åŠ è½½åå†»ç»“
        
        Returns:
            missing_keys, unexpected_keys
        """
        checkpoint = torch.load(path, map_location='cpu')
        
        # éªŒè¯é…ç½®
        if 'config' in checkpoint:
            config = checkpoint['config']
            if config.get('num_agents') != self.num_agents:
                print(f"âš ï¸ Warning: num_agents mismatch! "
                      f"Checkpoint: {config['num_agents']}, Current: {self.num_agents}")
        
        # åŠ è½½å‚æ•°
        missing, unexpected = self.modules_dict[module_name].load_state_dict(
            checkpoint['state_dict'], 
            strict=strict
        )
        
        if freeze:
            self.freeze_module(module_name)
        
        print(f"âœ… Loaded '{module_name}' checkpoint from {path}")
        if missing:
            print(f"  Missing keys: {missing}")
        if unexpected:
            print(f"  Unexpected keys: {unexpected}")
        
        return missing, unexpected

    def save_all_modules(self, save_dir):
        """ä¿å­˜æ‰€æœ‰æ¨¡å—åˆ°æŒ‡å®šç›®å½•"""
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        for module_name in self.modules_dict.keys():
            save_path = os.path.join(save_dir, f"{module_name}.pth")
            self.save_module_checkpoint(module_name, save_path)

    def load_pretrained_low_level(self, path, freeze=True):
        """
        æ™ºèƒ½åŠ è½½å‡½æ•°ï¼šæ”¯æŒåŠ è½½ 'æ¨¡å—åŒ–Checkpoint' æˆ– 'å®Œæ•´è®­ç»ƒCheckpoint'
        """
        print(f"ğŸ”„ Loading low-level params from {path}...")
        checkpoint = torch.load(path, map_location='cpu')
        
        low_level_state_dict = {}
        
        # === æƒ…å†µ A: è¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ– Checkpoint ===
        if 'state_dict' in checkpoint:
            print("  Type: Module Checkpoint")
            low_level_state_dict = checkpoint['state_dict']
            
        # === æƒ…å†µ B: è¿™æ˜¯ä¸€ä¸ªå®Œæ•´è®­ç»ƒ Checkpoint ===
        elif 'models' in checkpoint:
            print("  Type: Full Training Checkpoint (extracting params...)")
            full_state_dict = checkpoint['models'][0]
            
            # â­ å…³é”®ä¿®æ”¹ï¼šæ˜ç¡®åº•å±‚ç½‘ç»œçš„é”®åå‰ç¼€
            # æ—§ä»£ç ä¸­ï¼Œåº•å±‚ç½‘ç»œçš„é”®ååº”è¯¥æ˜¯ 'low_agent_encoder.*', 'value_head.*' ç­‰
            target_keys = [
                'low_agent_encoder',  # â† è¿™æ˜¯åº•å±‚ç¼–ç å™¨çš„çœŸæ­£åå­—
                'value_head',
                'policy_head',
                'dist'
            ]
            
            for key, value in full_state_dict.items():
                # å»é™¤å¯èƒ½çš„ 'modules_dict.low_level.' å‰ç¼€ï¼ˆå¦‚æœæ˜¯æ–°ç‰ˆä»£ç ä¿å­˜çš„ï¼‰
                clean_key = key.replace('modules_dict.low_level.', '')
                
                # æ£€æŸ¥æ˜¯å¦å±äºåº•å±‚ç½‘ç»œï¼ˆå¿…é¡»å®Œæ•´åŒ¹é…å‰ç¼€ï¼‰
                if any(clean_key.startswith(prefix) for prefix in target_keys):
                    # â­ å¦‚æœæ˜¯æ—§ä»£ç ï¼Œéœ€è¦å°† 'low_agent_encoder' æ˜ å°„ä¸º 'encoder'
                    # å› ä¸ºæ–°ä»£ç ä¸­åº•å±‚æ¨¡å—å†…éƒ¨çš„åå­—æ˜¯ 'encoder'
                    final_key = clean_key.replace('low_agent_encoder', 'encoder')
                    low_level_state_dict[final_key] = value
                    
        else:
            raise ValueError(f"Unknown checkpoint format! Keys found: {list(checkpoint.keys())}")

        # åŠ è½½å‚æ•°
        missing, unexpected = self.modules_dict['low_level'].load_state_dict(
            low_level_state_dict, 
            strict=False 
        )
        
        if freeze:
            self.freeze_module('low_level')
            
        print(f"âœ… Low-level loaded. Missing keys: {len(missing)}, Unexpected keys: {len(unexpected)}")
        if missing:
            print(f"  âš ï¸ Missing: {missing}")
        if unexpected:
            print(f"  âš ï¸ Unexpected: {unexpected}")
        
        return missing, unexpected
        """
        ä¾¿æ·å‡½æ•°ï¼šåŠ è½½é¢„è®­ç»ƒçš„åº•å±‚ç½‘ç»œ
        
        Args:
            path: checkpoint è·¯å¾„
            freeze: æ˜¯å¦å†»ç»“å‚æ•°
        """
        # return self.load_module_checkpoint('low_level', path, strict=False, freeze=freeze)

    def get_trainable_params_by_modules(self, module_names, learning_rates=None):
        """
        è·å–å¤šä¸ªæ¨¡å—çš„å‚æ•°ç»„ï¼ˆç”¨äºä¼˜åŒ–å™¨ï¼‰
        
        Args:
            module_names: æ¨¡å—åç§°åˆ—è¡¨
            learning_rates: å¯¹åº”çš„å­¦ä¹ ç‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            param_groups for optimizer
        
        Example:
            >>> param_groups = model.get_trainable_params_by_modules(
            ...     ['high_level', 'high_critic', 'low_level'],
            ...     [3e-4, 3e-4, 1e-5]  # åº•å±‚ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
            ... )
            >>> optimizer = torch.optim.Adam(param_groups)
        """
        param_groups = []
        
        if learning_rates is None:
            learning_rates = [None] * len(module_names)
        
        for module_name, lr in zip(module_names, learning_rates):
            params = self.get_module_params(module_name)
            if lr is not None:
                param_groups.append({'params': params, 'lr': lr})
            else:
                param_groups.append({'params': params})
        
        return param_groups

    # ==================== å‰å‘ä¼ æ’­æ¥å£ï¼ˆä¿æŒå…¼å®¹ï¼‰====================
    
    @property
    def low_agent_encoder(self):
        return self.modules_dict['low_level']['encoder']
    
    @property
    def value_head(self):
        return self.modules_dict['low_level']['value_head']
    
    @property
    def policy_head(self):
        return self.modules_dict['low_level']['policy_head']
    
    @property
    def dist(self):
        return self.modules_dict['low_level']['dist']
    
    @property
    def map_conv1(self):
        return self.modules_dict['high_level']['map_conv1']
    
    @property
    def map_conv2(self):
        return self.modules_dict['high_level']['map_conv2']
    
    @property
    def map_conv3(self):
        return self.modules_dict['high_level']['map_conv3']
    
    @property
    def vec_mlp(self):
        return self.modules_dict['high_level']['vec_mlp']
    
    @property
    def decision_head(self):
        return self.modules_dict['high_level']['decision_head']
    
    @property
    def decoder_fuse(self):
        return self.modules_dict['high_level']['decoder_fuse']
    
    @property
    def decoder_up1(self):
        return self.modules_dict['high_level']['decoder_up1']
    
    @property
    def decoder_up2(self):
        return self.modules_dict['high_level']['decoder_up2']
    
    @property
    def decoder_out(self):
        return self.modules_dict['high_level']['decoder_out']
    
    @property
    def critic_map_backbone(self):
        return self.modules_dict['high_critic']['map_backbone']
    
    @property
    def critic_map_compress(self):
        return self.modules_dict['high_critic']['map_compress']
    
    @property
    def critic_agent_encoder(self):
        return self.modules_dict['high_critic']['agent_encoder']
    
    @property
    def critic_fusion_layer(self):
        return self.modules_dict['high_critic']['fusion_layer']
    
    @property
    def critic_value_out_heads(self):
        return self.modules_dict['high_critic']['value_heads']
    
    @property
    def critic_map_flat_dim(self):
        return 64 * 6 * 6
    
    @property
    def ego_node_encoder(self):
        return self.modules_dict['high_level']['ego_node_encoder']
    
    @property
    def teammate_node_encoder(self):
        return self.modules_dict['high_level']['teammate_node_encoder']
    
    @property
    def explore_node_encoder(self):
        return self.modules_dict['high_level']['explore_node_encoder']
    
    @property
    def landmark_node_encoder(self):
        return self.modules_dict['high_level']['landmark_node_encoder']
    
    @property
    def explore_node_linear(self):
        return self.modules_dict['high_level']['explore_node_linear']
    
    @property
    def landmark_node_linear(self):
        return self.modules_dict['high_level']['landmark_node_linear']
    
    @property
    def linear_ln(self):
        return self.modules_dict['high_level']['linear_ln']
    
    @property
    def edge_encoder(self):
        return self.modules_dict['high_level']['edge_encoder']
    
    @property
    def q_proj(self):
        return self.modules_dict['high_level']['q_proj']
    
    @property
    def k_proj(self):
        return self.modules_dict['high_level']['k_proj']
    
    @property
    def v_proj(self):
        return self.modules_dict['high_level']['v_proj']
    
    @property
    def node_selection_head(self):
        return self.modules_dict['high_level']['node_selection_head']
    
    @property
    def attn_dim(self):
        return 64

    def get_explore_nodes(self, top_k, rrt_max_iter, vec_inp, map_inp, agent_indices=None, deterministic=False):
        """
        vec_inp: [Batch, num_agents, 4]ï¼Œä¸–ç•Œåæ ‡ï¼š[x_pos, y_pos, x_goal, y_goal]
        map_inp: [2, Batch, num_agents, H, W], (0: entropy, 1: voronoi_mask)
        agent_indices: [N] å¯é€‰ï¼Œä»…åœ¨éƒ¨åˆ†æ™ºèƒ½ä½“æ›´æ–°æ—¶æä¾›
        """

        # 1. é€šè¿‡RRTç”Ÿæˆå€™é€‰ç›®æ ‡ç‚¹
        ## è¾“å…¥vec_inp:[Batch, 2], map_inp:[2,Batch, H, W]
        ## è¾“å‡ºå€™é€‰ç›®æ ‡ç‚¹B_candidate:[Batch, K, 3]ï¼Œç¦»æ•£æ …æ ¼åæ ‡
        ### vec_inpè½¬åŒ–ä¸ºç¦»æ•£æ …æ ¼åæ ‡
        B_pro = vec_inp.size(0) 
        if agent_indices is not None:
            # agent_indices: [B_pro]
            batch_idx = torch.arange(B_pro, device=vec_inp.device)
            
            # [B_pro, num_agents, 4] -> [B_pro, 1, 4]
            update_nodes = vec_inp[batch_idx, agent_indices].unsqueeze(1)
            
            # [2, B_pro, num_agents, H, W] -> [B_pro, 1, H, W]
            voronoi_np = map_inp[1, batch_idx, agent_indices].unsqueeze(1).detach().cpu().numpy().astype(bool)
            entropy_np = map_inp[0, batch_idx, agent_indices].unsqueeze(1).detach().cpu().numpy().astype(np.float32)
        else:
            update_nodes = vec_inp
            voronoi_np = map_inp[1].detach().cpu().numpy().astype(bool)
            entropy_np = map_inp[0].detach().cpu().numpy().astype(np.float32)

        B_agents = update_nodes.size(1) # å¦‚æœæŒ‡å®šäº†indicesï¼Œè¿™é‡Œæ˜¯1ï¼›å¦åˆ™æ˜¯num_agents        

        # update_nodes: [B_pro, B_agents, 4] -> é€‰å–ä½ç½®éƒ¨åˆ† [B_pro*B_agents, 2]
        starte_nodes = self._world_to_grid_torch(update_nodes.view(-1, update_nodes.size(2))[:, :2], H=100, W=100)  # [B_pro*B_agents, 2] æ•´æ•°æ …æ ¼åæ ‡
        voronoi_inp = voronoi_np.reshape(-1, voronoi_np.shape[-2], voronoi_np.shape[-1])  # [B_pro*B_agents, H, W]
        entropy_inp = entropy_np.reshape(-1, entropy_np.shape[-2], entropy_np.shape[-1])  # [B_pro*B_agents, H, W]
        
        batch_rtt = plan_batch(starte_nodes, voronoi_inp, entropy_inp, max_iterations=rrt_max_iter, top_k=top_k)  # [B_pro*B_agents, K, 3]
        batch_rtt = torch.tensor(batch_rtt, dtype=torch.float32, device=vec_inp.device).view(B_pro, B_agents, -1, 3)  # [B_pro, B_agents, K, 3]
        
        ## è½¬ä¸ºä¸–ç•Œåæ ‡
        explore_nodes_world = self._grid_to_world_torch(batch_rtt[..., :2], H=100, W=100)  # [B_pro, B_agents, K, 2]
        
        # update_nodes: [B_pro, B_agents, 4]ï¼Œå‰ä¸¤ç»´æ˜¯ ego çš„ä½ç½®
        ego_positions = update_nodes[..., :2]  # [B_pro, B_agents, 2]
        
        # å¹¿æ’­å‡æ³•ï¼šexplore_nodes_world [B_pro, B_agents, K, 2] - ego_positions [B_pro, B_agents, 1, 2]
        relative_explore_positions = explore_nodes_world - ego_positions.unsqueeze(2)  # [B_pro, B_agents, K, 2]
        
        ## æ‹¼æ¥æˆå€™é€‰ç‚¹ç‰¹å¾ï¼ˆç°åœ¨å‰ä¸¤ç»´æ˜¯ç›¸å¯¹ä½ç½®ï¼‰
        explore_nodes = torch.cat([relative_explore_positions, batch_rtt[..., 2:3]], dim=-1)  # [B_pro, B_agents, K, 3]

        ## è¡¥å……å€™é€‰ç‚¹èŠ‚ç‚¹ç‰¹å¾ - Occupied Feature
        d0 = 0.3
        # æå–æ‰€æœ‰æ™ºèƒ½ä½“çš„ç›®æ ‡ä½ç½® [B_pro, num_agents, 2]
        all_goals = vec_inp[..., 2:4]        
        
        # è®¡ç®—è·ç¦»çŸ©é˜µï¼ˆè¿™é‡Œä»ç„¶ä½¿ç”¨ç»å¯¹ä½ç½®è®¡ç®—è·ç¦»ï¼‰
        # explore_nodes_world: [B_pro, B_agents, K, 2]ï¼ˆç»å¯¹ä½ç½®ï¼‰
        # all_goals: [B_pro, num_agents, 2]ï¼ˆç»å¯¹ä½ç½®ï¼‰
        dists = torch.norm(explore_nodes_world.unsqueeze(3) - all_goals.unsqueeze(1).unsqueeze(1), dim=-1)

        mask = torch.ones_like(dists, dtype=torch.bool)
        if agent_indices is not None:
            batch_idx = torch.arange(B_pro, device=vec_inp.device)
            mask[batch_idx, 0, :, agent_indices] = False
        else:
            diag_mask = torch.eye(B_agents, device=vec_inp.device).bool()
            mask = ~diag_mask.view(1, B_agents, 1, B_agents).expand(B_pro, B_agents, top_k, B_agents)

        dists = torch.where(mask, dists, torch.tensor(float('inf'), device=dists.device))

        valid_mask = (dists < d0)
        occ_vals = ((d0 - dists) / d0).pow(2)
        occ_vals = torch.where(valid_mask, occ_vals, torch.tensor(0.0, device=dists.device))
        
        occupied_feature = occ_vals.sum(dim=-1, keepdim=True) # [B_pro, B_agents, K, 1]
        
        # âœ… æœ€ç»ˆçš„ explore_nodes: [B_pro, B_agents, K, 4]
        # å‰ä¸¤ç»´æ˜¯ç›¸å¯¹ä½ç½®ï¼Œç¬¬ä¸‰ç»´æ˜¯ç†µå€¼ï¼Œç¬¬å››ç»´æ˜¯ occupied ç‰¹å¾
        explore_nodes = torch.cat([explore_nodes, occupied_feature], dim=-1)  # [B_pro, B_agents, K, 4]

        return explore_nodes

    def get_landmark_nodes(self, agent_positions, detected, detected_mask, linear_indices, all_masks=None):
        """
        agent_positions: [num_agents * num_processes, 2]ï¼Œä¸–ç•Œåæ ‡
        detected: [num_agents * num_processes, max_landmarks, 4]ï¼Œlandmarkç‰¹å¾ [x, y, utility, is_targeted]
        detected_mask: [num_agents * num_processes, max_landmarks, 1]ï¼Œå€™é€‰ç‚¹æœ‰æ•ˆæ©ç 
        linear_indices: [N]ï¼Œéœ€è¦å†³ç­–çš„æ™ºèƒ½ä½“çš„çº¿æ€§ç´¢å¼•
        all_masks: [num_agents * num_processes, 1] å¯é€‰ï¼Œæ™ºèƒ½ä½“æœ‰æ•ˆæ©ç 

        return:
            batch_landmark_nodes: List of Tensors, æ¯ä¸ª Tensor å½¢çŠ¶ä¸º [L_i, 4]ï¼Œä¸€å…±Batchä¸ªéœ€è¦å†³ç­–æ™ºèƒ½ä½“
                                  ç‰¹å¾ä¸º [relative_x, relative_y, utility, is_targeted]
            batch_landmark_nodes_masks: List of Tensors, æ¯ä¸ª Tensor å½¢çŠ¶ä¸º [L_i, 1]ï¼Œä¸€å…±Batchä¸ªéœ€è¦å†³ç­–æ™ºèƒ½ä½“
            å¦‚æœæœ‰æ™ºèƒ½ä½“æ²¡æœ‰æœ‰æ•ˆ landmarkï¼Œåˆ™å¯¹åº”çš„ Tensor å½¢çŠ¶ä¸º [1, 4] å’Œ [1, 1]ï¼Œå†…å®¹å…¨0ï¼ˆåç»­ä¼šè¢«maskæ‰ï¼‰
        """
        # ä»æ›´æ–°åçš„ landmark æ•°æ®ä¸­æå–å¯¹åº”çš„ landmarks
        batch_landmark_data = detected[linear_indices]  # [N, max_landmarks, 4]
        batch_landmark_mask = detected_mask[linear_indices]  # [N, max_landmarks, 1]
        
        # æå–å¯¹åº”æ™ºèƒ½ä½“çš„ä½ç½® [N, 2]
        ego_positions = agent_positions[linear_indices]  # [N, 2]

        # âœ… æ‰¹é‡è®¡ç®—ç›¸å¯¹ä½ç½®
        relative_pos = batch_landmark_data[:, :, :2] - ego_positions.unsqueeze(1)  # [N, max_L, 2]

        # æ ¹æ®å…¶ä»–æ™ºèƒ½ä½“æ˜¯å¦å·²ç»è¢«å‘ç°ï¼Œåˆ¤å®šå½“å‰çš„landmarkæ•ˆç”¨å€¼
        
        # æ›´æ–°landmarkæ•°æ®ï¼ˆä¿ç•™utilityå’Œis_targetedï¼‰
        batch_landmark_data_relative = torch.cat([
            relative_pos,
            batch_landmark_data[:, :, 2:]  # [N, max_L, 2] - utilityå’Œis_targeted
        ], dim=2)  # [N, max_L, 4]
        
        # ===== åŠ¨æ€è°ƒæ•´ landmark æ•ˆç”¨å€¼ï¼ˆåŸºäºåŒç¯å¢ƒä¸­çš„é€€ä¼‘æ™ºèƒ½ä½“æ•°é‡ï¼‰=====
        if all_masks is not None:
            # 1. è®¡ç®—æ¯ä¸ªæ™ºèƒ½ä½“æ‰€åœ¨çš„ç¯å¢ƒç´¢å¼•
            # linear_indices: [N]ï¼ŒèŒƒå›´ [0, num_agents * num_processes)
            # æ•°æ®ç»„ç»‡æ–¹å¼ï¼š[agent0_env0, agent0_env1, ..., agent0_env(P-1), agent1_env0, ...]
            # ç¯å¢ƒç´¢å¼• = linear_indices % num_processes
            num_processes = all_masks.size(0) // self.num_agents
            env_indices = linear_indices % num_processes  # [N]
            
            # 2. ä¸ºæ¯ä¸ªæ ·æœ¬è®¡ç®—å…¶æ‰€åœ¨ç¯å¢ƒä¸­çš„é€€ä¼‘æ™ºèƒ½ä½“æ•°é‡
            # all_masks: [num_agents * num_processes, 1]
            # é‡å¡‘ä¸º [num_agents, num_processes] ç„¶åè½¬ç½®ä¸º [num_processes, num_agents]
            all_masks_reshaped = all_masks.view(self.num_agents, num_processes).t()  # [P, A]
            
            # å¯¹äºæ¯ä¸ªç¯å¢ƒï¼Œç»Ÿè®¡æ— æ•ˆï¼ˆå·²é€€ä¼‘ï¼‰æ™ºèƒ½ä½“æ•°é‡
            # mask=1 è¡¨ç¤ºæœ‰æ•ˆï¼Œmask=0 è¡¨ç¤ºå·²é€€ä¼‘
            retired_counts_per_env = (all_masks_reshaped < 0.5).sum(dim=1)  # [P]
            
            # 3. æ ¹æ®ç¯å¢ƒç´¢å¼•è·å–å¯¹åº”çš„é€€ä¼‘æ™ºèƒ½ä½“æ•°é‡
            num_retired_agents = retired_counts_per_env[env_indices]  # [N]
            
            # 4. æå– is_targeted å’Œ landmark æœ‰æ•ˆæ€§æ©ç 
            is_targeted = batch_landmark_data_relative[:, :, 3]  # [N, max_L]
            valid_landmark_mask = batch_landmark_mask.squeeze(-1) > 0.5  # [N, max_L]
            
            # 5. å¯¹äºæœ‰æ•ˆä¸”å°šæœªè¢«è¿½è¸ªçš„ landmarkï¼Œè®¾ç½®æ•ˆç”¨å€¼ä¸ºè¯¥ç¯å¢ƒä¸­çš„é€€ä¼‘æ™ºèƒ½ä½“æ•°é‡
            untargeted_and_valid = (is_targeted < 0.5) & valid_landmark_mask  # [N, max_L]
            
            # 6. æ›´æ–°æ•ˆç”¨å€¼ï¼ˆå¹¿æ’­ num_retired_agents [N] åˆ° [N, max_L]ï¼‰
            vals = (num_retired_agents + 2).float().unsqueeze(1)  # [N, 1]
            batch_landmark_data_relative[:, :, 2] = torch.where(
                untargeted_and_valid,
                vals,
                batch_landmark_data_relative[:, :, 2]
            )
        
        # ===== æ•ˆç”¨å€¼è°ƒæ•´å®Œæˆ =====
        
        # âœ… ç›´æ¥è¿”å›tensorï¼Œè€Œä¸æ˜¯listï¼Œ[N, max_L, 4] å’Œ [N, max_L, 1]
        return batch_landmark_data_relative, batch_landmark_mask

    def get_edge_features(self, explore_nodes, landmark_nodes, landmark_node_masks, norm=False, max_distance=2.8):
        """
        è®¡ç®—è¾¹ç‰¹å¾
        
        Args:
            explore_nodes: [B, K, 4]ï¼Œå€™é€‰æ¢ç´¢ç‚¹ç‰¹å¾
            landmark_nodes: tensor: [B, Max_L, 4]ï¼Œlandmark ç‰¹å¾
            landmark_node_masks: tensor: [B, Max_L, 1]ï¼Œlandmark æœ‰æ•ˆæ©ç 
            norm: æ˜¯å¦å½’ä¸€åŒ–è·ç¦»ï¼Œé»˜è®¤Falseï¼ˆä¸å½’ä¸€åŒ–ï¼‰
            max_distance: å½’ä¸€åŒ–æ—¶çš„æœ€å¤§è·ç¦»ï¼Œé»˜è®¤2.8
            
        Returns:
            batch_ego_to_explore_edges: tensor: [N, K, 3]
            batch_ego_to_landmark_edges: tensor: [N, Max_L, 3]
            batch_ego_to_landmark_edge_masks: tensor: [N, Max_L, 1]
        """
        """å‘é‡åŒ–ç‰ˆæœ¬ - æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬"""
        B = explore_nodes.shape[0]
        K = explore_nodes.shape[1]

        # âœ… æ‰¹é‡è®¡ç®—exploreè¾¹ç‰¹å¾
        explore_relative_pos = explore_nodes[:, :, :2]  # [B, K, 2]
        distances = torch.norm(explore_relative_pos, dim=2, keepdim=True)  # [B, K, 1]

        if norm:
            d_feature = distances / max_distance
        else:
            d_feature = distances
        
        distances_safe = distances.clamp(min=1e-6)
        cos_theta = explore_relative_pos[:, :, 0:1] / distances_safe  # [B, K, 1]
        sin_theta = explore_relative_pos[:, :, 1:2] / distances_safe  # [B, K, 1]

        # [B, K, 3] - ç›´æ¥è¿”å›tensorè€Œä¸æ˜¯list
        batch_ego_to_explore_edges = torch.cat([d_feature, cos_theta, sin_theta], dim=2)

        # âœ… æ‰¹é‡å¤„ç†landmarkè¾¹ç‰¹å¾ï¼ˆä½¿ç”¨paddingï¼‰
        max_L = landmark_nodes.shape[1]
        
        # æ„å»ºpadded tensor
        landmark_relative_pos = landmark_nodes[:, :, :2]  # [B, Max_L, 2]
        distances_landmark = torch.norm(landmark_relative_pos, dim=2, keepdim=True)  # [B, Max_L, 1]

        if norm:
            d_feature_landmark = distances_landmark / max_distance
        else:
            d_feature_landmark = distances_landmark 

        distances_landmark_safe = distances_landmark.clamp(min=1e-6)
        cos_theta_landmark = landmark_relative_pos[:, :, 0:1] / distances_landmark_safe  # [B, Max_L, 1]
        sin_theta_landmark = landmark_relative_pos[:, :, 1:2] / distances_landmark_safe  # [B, Max_L, 1]    

        batch_ego_to_landmark_edges = torch.cat([d_feature_landmark, cos_theta_landmark, sin_theta_landmark], dim=2)  # [B, Max_L, 3]
        batch_ego_to_landmark_edge_masks = landmark_node_masks  # [B, Max_L, 1]
        
        return batch_ego_to_explore_edges, batch_ego_to_landmark_edges, batch_ego_to_landmark_edge_masks
    
    def landmark_team_gat(self, batch_ego_pos, batch_teammate_nodes, batch_teammate_masks,
                           batch_landmark_nodes, batch_landmark_node_masks,
                           landmark_node_feats):
        """
        ä½¿ç”¨é˜Ÿå‹èŠ‚ç‚¹å¯¹ landmark èŠ‚ç‚¹è¿›è¡Œ GAT èšåˆ
        Args:
            batch_ego_pos: [B, 2] ego ä½ç½®ï¼Œç”¨äºè®¡ç®—ç›¸å¯¹ä½ç½®
            batch_teammate_nodes: [B, num_agents, 5] é˜Ÿå‹èŠ‚ç‚¹ç‰¹å¾,[x, y, vel_x, vel_y, dist_to_goal]
            batch_teammate_masks: [B, num_agents, 1] é˜Ÿå‹èŠ‚ç‚¹æ©ç 
            batch_landmark_nodes: [B, Max_L, 4] landmark èŠ‚ç‚¹ç‰¹å¾ (ç›¸å¯¹egoçš„ä½ç½®)
            batch_landmark_node_masks: [B, Max_L, 1] landmark èŠ‚ç‚¹æ©ç 
            landmark_node_feats: [B, Max_L, 64] landmark èŠ‚ç‚¹åµŒå…¥ç‰¹å¾
        Returns:
            landmark_node_agg_feats: [B, Max_L, 64] èšåˆåçš„ landmark èŠ‚ç‚¹ç‰¹å¾
        """
        B = batch_landmark_nodes.size(0)
        max_L = batch_landmark_nodes.size(1)
        num_agents = batch_teammate_nodes.size(1)

        # 1. è®¡ç®—landmarkçš„ç»å¯¹ä¸–ç•Œåæ ‡
        # batch_landmark_nodes[:, :, :2] æ˜¯ç›¸å¯¹äºegoçš„ä½ç½® [B, Max_L, 2]
        # batch_ego_pos [B, 2] -> [B, 1, 2]
        landmark_abs_pos = batch_ego_pos.unsqueeze(1) + batch_landmark_nodes[:, :, :2]  # [B, Max_L, 2]
        
        # 2. æå–é˜Ÿå‹çš„ç»å¯¹ä½ç½®
        teammate_abs_pos = batch_teammate_nodes[:, :, :2]  # [B, num_agents, 2]
        
        # 3. è®¡ç®—é˜Ÿå‹ç›¸å¯¹äºæ¯ä¸ªlandmarkçš„ä½ç½®
        # teammate_abs_pos [B, 1, num_agents, 2] - landmark_abs_pos [B, Max_L, 1, 2]
        # => [B, Max_L, num_agents, 2]
        teammate_relative_to_landmark = teammate_abs_pos.unsqueeze(1) - landmark_abs_pos.unsqueeze(2)  # [B, Max_L, num_agents, 2]
        
        # 4. æ‹¼æ¥å…¶ä»–ç‰¹å¾ï¼ˆé€Ÿåº¦å’Œè·ç¦»ç›®æ ‡ï¼‰
        # batch_teammate_nodes[:, :, 2:] [B, num_agents, 3] -> [B, 1, num_agents, 3]
        teammate_other_feats = batch_teammate_nodes[:, :, 2:].unsqueeze(1).expand(B, max_L, num_agents, 3)  # [B, Max_L, num_agents, 3]
        
        # 5. ç»„åˆæˆå®Œæ•´çš„é˜Ÿå‹èŠ‚ç‚¹ç‰¹å¾ [B, Max_L, num_agents, 5]
        teammate_nodes_per_landmark = torch.cat([
            teammate_relative_to_landmark,  # [B, Max_L, num_agents, 2]
            teammate_other_feats            # [B, Max_L, num_agents, 3]
        ], dim=-1)  # [B, Max_L, num_agents, 5]

        # 6. ç¼–ç é˜Ÿå‹èŠ‚ç‚¹ç‰¹å¾
        teammate_node_feats = self.teammate_node_encoder(
            teammate_nodes_per_landmark.contiguous().view(B * max_L * num_agents, -1)
        ).view(B, max_L, num_agents, -1)  # [B, Max_L, num_agents, 64]

        # 7. é€šè¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å¯¹landmarkèŠ‚ç‚¹è¿›è¡Œèšåˆ
        # landmarkèŠ‚ç‚¹ä½œä¸ºQueryï¼Œé˜Ÿå‹èŠ‚ç‚¹ä½œä¸ºKeyå’ŒValue
        
        # 7.1 å‡†å¤‡Queryï¼šlandmarkèŠ‚ç‚¹ç‰¹å¾ [B, Max_L, 64] -> [B, Max_L, 1, 64]
        landmark_query = landmark_node_feats.unsqueeze(2)  # [B, Max_L, 1, 64]
        
        # 7.2 å‡†å¤‡Keyå’ŒValueï¼šé˜Ÿå‹èŠ‚ç‚¹ç‰¹å¾ [B, Max_L, num_agents, 64]
        teammate_key = teammate_node_feats  # [B, Max_L, num_agents, 64]
        teammate_value = teammate_node_feats  # [B, Max_L, num_agents, 64]
        
        # 7.3 è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scale = math.sqrt(64)  # attn_dim = 64
        attn_scores = torch.matmul(landmark_query, teammate_key.transpose(-2, -1)) / scale  # [B, Max_L, 1, num_agents]
        
        # 7.4 åº”ç”¨é˜Ÿå‹èŠ‚ç‚¹maskï¼ˆå°†æ— æ•ˆé˜Ÿå‹çš„æ³¨æ„åŠ›åˆ†æ•°è®¾ä¸º-infï¼‰
        # batch_teammate_masks: [B, num_agents, 1] -> [B, 1, 1, num_agents]
        teammate_mask_expanded = batch_teammate_masks.transpose(1, 2).unsqueeze(1)  # [B, 1, 1, num_agents]
        teammate_mask_expanded = teammate_mask_expanded.expand(B, max_L, 1, num_agents)  # [B, Max_L, 1, num_agents]
        attn_scores = attn_scores.masked_fill(teammate_mask_expanded < 0.5, -1e9)
        
        # 7.5 åº”ç”¨landmarkèŠ‚ç‚¹maskï¼ˆæ— æ•ˆçš„landmarkä¸è®¡ç®—æ³¨æ„åŠ›ï¼‰
        # batch_landmark_node_masks: [B, Max_L, 1] -> [B, Max_L, 1, 1]
        landmark_mask_expanded = batch_landmark_node_masks.unsqueeze(-1)  # [B, Max_L, 1, 1]
        attn_scores = attn_scores.masked_fill(landmark_mask_expanded < 0.5, -1e9)
        
        # 7.6 Softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attn_weights = torch.softmax(attn_scores, dim=-1)  # [B, Max_L, 1, num_agents]

        # æ›´ä¸¥è°¨çš„åšæ³•ï¼šé‡æ–°åº”ç”¨ Mask åˆ° weights ä¸Šï¼Œç¡®ä¿è¢« mask çš„æƒé‡ç»å¯¹ä¸º 0
        attn_weights = attn_weights * teammate_mask_expanded
        
        # 7.7 åŠ æƒæ±‚å’Œå¾—åˆ°èšåˆç‰¹å¾
        landmark_aggregated = torch.matmul(attn_weights, teammate_value).squeeze(2)  # [B, Max_L, 64]

        # æ„é€ ä¸€ä¸ªèšåˆ Maskï¼šå¦‚æœåœ¨è¯¥ç»´åº¦ä¸Šæ‰€æœ‰ teammate éƒ½è¢« maskï¼Œåˆ™è¯¥ aggregate æ— æ•ˆ
        # teammate_mask_expanded: [B, Max_L, 1, num_agents]
        # valid_context_mask: [B, Max_L, 1] -> True è¡¨ç¤ºè‡³å°‘æœ‰ä¸€ä¸ªé˜Ÿå‹
        valid_context_mask = (teammate_mask_expanded.sum(dim=-1) > 0).float() # [B, Max_L, 1]
        
        # å¼ºåˆ¶æ¸…é›¶æ— æ•ˆçš„èšåˆ
        landmark_aggregated = landmark_aggregated * valid_context_mask
        
        # 7.8 æ®‹å·®è¿æ¥ï¼šèšåˆç‰¹å¾ + åŸå§‹landmarkç‰¹å¾
        landmark_node_agg_feats = landmark_node_feats + landmark_aggregated  # [B, Max_L, 64]
        
        return landmark_node_agg_feats

    def get_high_level_goal(self, batch_ego_nodes, 
                            batch_teammate_nodes, batch_teammate_masks,
                            batch_explore_nodes, batch_ego_to_explore_edges, 
                            batch_landmark_nodes, batch_landmark_node_masks, 
                            batch_ego_to_landmark_edges, batch_ego_to_landmark_edge_masks, 
                            deterministic=False):
        """
        ç»Ÿä¸€ä»æ‰€æœ‰å€™é€‰èŠ‚ç‚¹ï¼ˆexplore + landmarkï¼‰ä¸­é€‰æ‹©ä¸€ä¸ªç›®æ ‡
        
        Args:
            batch_ego_nodes: [B, 5] [x, y, vel_x, vel_y, battery]
            batch_teammate_nodes: [B, num_agents, 5] [x, y, vel_x, vel_y, dist_to_goal]
            batch_teammate_masks: [B, num_agents, 1] (1=æœ‰æ•ˆ, 0=æ— æ•ˆ)
            batch_explore_nodes: [B, K, 4] [relative_x, relative_y, utility, occupied]
            batch_ego_to_explore_edges: [B, K, 3] [d, cosÎ¸, sinÎ¸]
            batch_landmark_nodes: [B, Max_L, 4] [relative_x, relative_y, utility, is_targeted]
            batch_landmark_node_masks: [B, Max_L, 1] (1=æœ‰æ•ˆ, 0=æ— æ•ˆ)
            batch_ego_to_landmark_edges: [B, Max_L, 3]
            batch_ego_to_landmark_edge_masks: [B, Max_L, 1]
            deterministic: bool
        
        Returns:
            dict:
                action_modes: [B, 1] (0=explore, 1=landmark) è¢«é€‰ä¸­èŠ‚ç‚¹çš„ç±»å‹
                waypoints: [B, 2] ç»å¯¹ä¸–ç•Œåæ ‡
                decision_log_probs: [B, 1] èŠ‚ç‚¹é€‰æ‹©çš„ log_prob
                map_log_probs: [B, 1] åŒä¸Šï¼ˆä¿æŒæ¥å£å…¼å®¹ï¼‰
        """
        B = batch_ego_nodes.size(0)
        K = batch_explore_nodes.size(1)
        
        # ===== 1. èŠ‚ç‚¹å’Œè¾¹çš„ç‰¹å¾åµŒå…¥ =====
        # 1.1 EgoèŠ‚ç‚¹åµŒå…¥
        ego_node_feats = self.ego_node_encoder(batch_ego_nodes)  # [B, 64]
        
        # 1.2 ExploreèŠ‚ç‚¹åµŒå…¥
        explore_node_feats = self.explore_node_encoder(
            batch_explore_nodes.view(B * K, -1)
        ).view(B, K, -1)  # [B, K, 64]
 
        # 1.3 LandmarkèŠ‚ç‚¹GATèšåˆ
        max_L = batch_landmark_nodes.size(1)  # [B, Max_L, 4]

        # ä¸€æ¬¡æ€§ç¼–ç æ‰€æœ‰ landmark èŠ‚ç‚¹ [B, Max_L, 4] -> [B, Max_L, 64]
        landmark_node_feats = self.landmark_node_encoder(
            batch_landmark_nodes.view(B * max_L, -1)
        ).view(B, max_L, -1)  # [B, Max_L, 64]

        # landmark å’Œ é˜Ÿå‹èŠ‚ç‚¹è¿›è¡Œèšåˆ
        landmark_node_agg_feats = self.landmark_team_gat(
            batch_ego_nodes[:, :2],  # åªä¼ é€’ ego çš„ä½ç½®ç”¨äºè®¡ç®—ç›¸å¯¹ä½ç½®
            batch_teammate_nodes, batch_teammate_masks,
            batch_landmark_nodes, batch_landmark_node_masks, landmark_node_feats
        )  # [B, Max_L, 64]

        # 1.4 å¯¹exploreå’ŒlandmarkèŠ‚ç‚¹ç‰¹å¾è¿›è¡Œçº¿æ€§å˜æ¢å’ŒLayerNorm
        explore_node_feats = self.explore_node_linear(
            explore_node_feats.view(B * K, -1)
        ).view(B, K, -1)  # [B, K, 64]

        explore_node_feats = self.linear_ln(explore_node_feats)  # LayerNorm

        landmark_node_feats = self.landmark_node_linear(
            landmark_node_agg_feats.view(B * max_L, -1)
        ).view(B, max_L, -1)  # [B, Max_L, 64]

        landmark_node_feats = self.linear_ln(landmark_node_feats)  # LayerNorm

        # 1.5 ç¼–ç æ‰€æœ‰ landmark è¾¹ [B, Max_L, 3] -> [B, Max_L, 32]
        landmark_edge_feats = self.edge_encoder(
            batch_ego_to_landmark_edges.view(B * max_L, -1)
        ).view(B, max_L, -1)  # [B, Max_L, 32]

        # ç¼–ç  explore è¾¹ [B, K, 3] -> [B, K, 32]
        explore_edge_feats = self.edge_encoder(
            batch_ego_to_explore_edges.view(B * K, -1)
        ).view(B, K, -1)  # [B, K, 32]

        # ===== 2. æ„å»ºç»Ÿä¸€çš„å€™é€‰èŠ‚ç‚¹é›†åˆ =====
        # å°† explore å’Œ landmark åˆå¹¶ä¸ºä¸€ä¸ªç»Ÿä¸€çš„èŠ‚ç‚¹é›†
        # æ€»èŠ‚ç‚¹æ•° = K (explore) + max_L (landmark)
        total_nodes = K + max_L
        
        # 2.1 å‡†å¤‡ Query
        q = self.q_proj(ego_node_feats).unsqueeze(1)  # [B, 1, 64]
        
        # 2.2 åˆå¹¶æ‰€æœ‰èŠ‚ç‚¹çš„ Key å’Œ Value
        # åˆå§‹åŒ–ç»Ÿä¸€çš„ K/V çŸ©é˜µ: [B, K+max_L, 64]
        unified_k = torch.zeros(B, total_nodes, self.attn_dim, device=ego_node_feats.device)
        unified_v = torch.zeros(B, total_nodes, self.attn_dim, device=ego_node_feats.device)
        unified_mask = torch.zeros(B, total_nodes, device=ego_node_feats.device, dtype=torch.bool)
        
        # å­˜å‚¨ç›¸å¯¹åæ ‡ç”¨äºåç»­è¾“å‡º
        unified_relative_pos = torch.zeros(B, total_nodes, 2, device=ego_node_feats.device)

        # èŠ‚ç‚¹ç±»å‹æ ‡ç­¾: 0=explore, 1=landmark
        node_type_labels = torch.zeros(B, total_nodes, device=ego_node_feats.device, dtype=torch.long)
        
        # å¡«å…… explore èŠ‚ç‚¹ (ç´¢å¼• 0 ~ K-1)
        explore_kv = torch.cat([explore_node_feats, explore_edge_feats], dim=-1)  # [B, K, 96]
        unified_k[:, :K, :] = self.k_proj(explore_kv.view(B * K, -1)).view(B, K, -1)
        unified_v[:, :K, :] = self.v_proj(explore_kv.view(B * K, -1)).view(B, K, -1)
        unified_mask[:, :K] = True  # explore èŠ‚ç‚¹å…¨éƒ¨æœ‰æ•ˆ
        unified_relative_pos[:, :K, :] = batch_explore_nodes[:, :, :2]  # ç›¸å¯¹åæ ‡
        node_type_labels[:, :K] = 0  # explore ç±»å‹
        
        # å¡«å…… landmark èŠ‚ç‚¹ (ç´¢å¼• K ~ K+max_L-1)
        # æ‹¼æ¥èŠ‚ç‚¹å’Œè¾¹ç‰¹å¾ [B, Max_L, 64] + [B, Max_L, 32] -> [B, Max_L, 96]
        lm_kv = torch.cat([landmark_node_feats, landmark_edge_feats], dim=-1)
        # æŠ•å½±ä¸º K/V [B, Max_L, 96] -> [B, Max_L, 64]
        unified_k[:, K:K+max_L, :] = self.k_proj(lm_kv.view(B * max_L, -1)).view(B, max_L, -1)
        unified_v[:, K:K+max_L, :] = self.v_proj(lm_kv.view(B * max_L, -1)).view(B, max_L, -1)
        
        # æ„å»ºæœ‰æ•ˆæ©ç ï¼šmask æœ‰æ•ˆ ä¸” æœªè¢«è¿½è¸ª
        valid_mask = batch_landmark_node_masks[:, :, 0] > 0.5  # [B, Max_L]
        not_targeted = batch_landmark_nodes[:, :, 3] < 0.5    # [B, Max_L]
        combined_mask = valid_mask & not_targeted              # [B, Max_L]

        # å¦‚æœvalid_maskæŸè¡Œå…¨éƒ¨æœ‰æ•ˆï¼Œåˆ™è¯´æ˜å·²ç»æ‰¾åˆ°å…¨éƒ¨landmarkï¼Œåˆ™è¯¥æ ·æœ¬ä¸éœ€è¦æ¢ç´¢èŠ‚ç‚¹
        all_landmarks_found = valid_mask.all(dim=1)  # [B] bool tensor
        if all_landmarks_found.any():
            unified_mask[all_landmarks_found, :K] = False
        
        unified_mask[:, K:K+max_L] = combined_mask
        unified_relative_pos[:, K:K+max_L, :] = batch_landmark_nodes[:, :, :2]
        node_type_labels[:, K:K+max_L] = 1

        # ===== 3. æ³¨æ„åŠ›æœºåˆ¶ =====
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scale = math.sqrt(self.attn_dim)
        attn_scores = (q @ unified_k.transpose(1, 2)) / scale  # [B, 1, total_nodes]
        
        # åº”ç”¨ maskï¼ˆæ— æ•ˆèŠ‚ç‚¹è®¾ä¸º -infï¼‰
        attn_scores = attn_scores.masked_fill(~unified_mask.unsqueeze(1), -1e9)
        
        # Softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attn_weights = torch.softmax(attn_scores, dim=-1)  # [B, 1, total_nodes]
        
        # æ³¨æ„åŠ›åŠ æƒæ±‚å’Œ
        context = (attn_weights @ unified_v).squeeze(1)  # [B, 64]
        
        # ===== 4. èŠ‚ç‚¹é€‰æ‹© =====
        # å¯¹æ‰€æœ‰èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†
        selection_logits = self.node_selection_head(unified_v).squeeze(-1)  # [B, total_nodes]
        
        # åº”ç”¨ mask
        selection_logits = selection_logits.masked_fill(~unified_mask, -1e9)
        
        # æ„å»ºåˆ†ç±»åˆ†å¸ƒ
        node_dist = TorchCategorical(logits=selection_logits)
        
        # é‡‡æ ·æˆ–é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹
        if deterministic:
            selected_idx = torch.argmax(selection_logits, dim=-1)  # [B]
        else:
            selected_idx = node_dist.sample()  # [B] or [B, 1]
        
        # ç¡®ä¿ selected_idx æ˜¯ [B] å½¢çŠ¶
        selected_idx = selected_idx.view(B)  # [B]
        
        # è®¡ç®— log_prob
        node_log_prob = node_dist.log_prob(selected_idx)  # [B]
        
        # ===== 5. æå–é€‰ä¸­èŠ‚ç‚¹çš„ä¿¡æ¯ =====
        batch_indices = torch.arange(B, device=ego_node_feats.device)
        
        # 5.1 èŠ‚ç‚¹ç±»å‹ (0=explore, 1=landmark)
        selected_type = node_type_labels[batch_indices, selected_idx]  # [B]
        
        # 5.2 ç›¸å¯¹åæ ‡
        selected_relative_pos = unified_relative_pos[batch_indices, selected_idx, :]  # [B, 2]
        
        # 5.3 è½¬æ¢ä¸ºç»å¯¹ä¸–ç•Œåæ ‡
        ego_pos = batch_ego_nodes[:, :2]  # [B, 2]
        waypoints_world = ego_pos + selected_relative_pos  # [B, 2]
        
        # ===== 6. è¿”å›ç»“æœ =====
        return {
            "action_modes": selected_type.unsqueeze(-1),       # [B, 1] èŠ‚ç‚¹ç±»å‹
            "waypoints": waypoints_world,                      # [B, 2] ç»å¯¹ä¸–ç•Œåæ ‡
            "node_log_probs": node_log_prob.unsqueeze(-1), # [B, 1] èŠ‚ç‚¹é€‰æ‹© log_prob
        }
    
    def get_high_value(self, map_inp, agent_states):
        """
        è®¡ç®—æ¯ä¸ªæ™ºèƒ½ä½“çš„çŠ¶æ€ä»·å€¼
        
        Args:
            map_inp: [B, 3, H, W] å…¨å±€åœ°å›¾ (entropy_map, heatmap, landmark_heatmap)
            agent_states: [B, num_agents, 4] æ™ºèƒ½ä½“çŠ¶æ€ [x, y, x_g, y_g]
        
        Returns:
            values: [B, num_agents] æ¯ä¸ªæ™ºèƒ½ä½“çš„ä»·å€¼ä¼°è®¡
        """
        B = map_inp.size(0)
        num_agents = agent_states.size(1)

        # 1. å…¨å±€åœ°å›¾ç‰¹å¾æå– [B, 3, H, W] -> [B, 256]
        f_map = self.critic_map_backbone(map_inp)  # [B, 64, 6, 6]
        f_map_flat = f_map.view(B, -1)  # [B, 64*6*6]
        f_global = self.critic_map_compress(f_map_flat)  # [B, 256]

        # 2. ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“è®¡ç®—ä»·å€¼
        values = []
        for agent_idx in range(num_agents):
            # 2.1 æå–è¯¥æ™ºèƒ½ä½“çš„çŠ¶æ€ [B, 4]
            agent_state = agent_states[:, agent_idx, :]  # [B, 4]
            
            # 2.2 ç¼–ç æ™ºèƒ½ä½“çŠ¶æ€ [B, 4] -> [B, 64]
            f_agent = self.critic_agent_encoder(agent_state)  # [B, 64]
            
            # 2.3 èåˆå…¨å±€ç‰¹å¾å’Œæ™ºèƒ½ä½“ç‰¹å¾ [B, 256] + [B, 64] -> [B, 320]
            fused = torch.cat([f_global, f_agent], dim=1)  # [B, 320]
            
            # 2.4 é€šè¿‡èåˆå±‚ [B, 320] -> [B, 128]
            h = self.critic_fusion_layer(fused)  # [B, 128]
            
            # 2.5 é€šè¿‡è¯¥æ™ºèƒ½ä½“çš„ç‹¬ç«‹ä»·å€¼å¤´ [B, 128] -> [B, 1]
            value = self.critic_value_out_heads[agent_idx](h)  # [B, 1]
            values.append(value)
        
        # 3. æ‹¼æ¥æ‰€æœ‰æ™ºèƒ½ä½“çš„ä»·å€¼ [B, num_agents]
        values = torch.cat(values, dim=1)  # [B, num_agents]

        return values

    def _low_value(self, x):
        return self.value_head(x) # h_dim -> h_dim -> 1

    def _low_policy(self, x): # h_dim -> h_dim
        return self.policy_head(x)

    def data_processing_low_level(self, inp, goals):
        # inp: [num_agents*batch_size, dim_o]
        # goals: [num_agents*batch_size, 2], assigned goals for agents

        batch_size = inp.size(0)

        # æå–é€Ÿåº¦ [batch_size, 2]
        velocities = inp[:, 0:2]

        # æå–è‡ªèº«ä½ç½® [batch_size, 2]
        self_pos = inp[:, 2:4]

        # è®¡ç®—ä¸ç›®æ ‡çš„ç›¸å¯¹ä½ç½® [batch_size, 2]
        relative_goal_pos = goals - self_pos

        # æå–å…¶ä»–æ™ºèƒ½ä½“çš„ç»å¯¹ä½ç½®
        # ä» inp ä¸­æå–ï¼šè·³è¿‡é€Ÿåº¦(2)ã€è‡ªèº«ä½ç½®(2)ã€landmarks(num_agents*2)
        other_agents_start_idx = 4 + self.num_agents * 2
        other_agents_pos = inp[:, other_agents_start_idx:other_agents_start_idx + (self.num_agents - 1) * 2]

        # å°†å…¶ä»–æ™ºèƒ½ä½“ä½ç½®é‡å¡‘ä¸º [batch_size, num_agents-1, 2]
        other_agents_pos = other_agents_pos.view(batch_size, self.num_agents - 1, 2)

        # è®¡ç®—ä¸å…¶ä»–æ™ºèƒ½ä½“çš„ç›¸å¯¹ä½ç½®
        # æ‰©å±• self_pos ä»¥ä¾¿å¹¿æ’­: [batch_size, 1, 2]
        self_pos_expanded = self_pos.unsqueeze(1)

        # ç›¸å¯¹ä½ç½® [batch_size, num_agents-1, 2]
        relative_other_agents_pos = other_agents_pos - self_pos_expanded

        # å±•å¹³å…¶ä»–æ™ºèƒ½ä½“çš„ç›¸å¯¹ä½ç½® [batch_size, (num_agents-1)*2]
        relative_other_agents_pos = relative_other_agents_pos.view(batch_size, -1)

        # æ‹¼æ¥æ–°çš„è§‚æµ‹å‘é‡
        # [batch_size, 2 + 2 + (num_agents-1)*2]
        new_inp = torch.cat([
            velocities,                    # é€Ÿåº¦ (2)
            relative_goal_pos,             # ä¸ç›®æ ‡çš„ç›¸å¯¹ä½ç½® (2)
            relative_other_agents_pos      # ä¸å…¶ä»–æ™ºèƒ½ä½“çš„ç›¸å¯¹ä½ç½® ((num_agents-1)*2)
        ], dim=1)

        return new_inp

    def low_level_act(self, inp, goals, deterministic=False):
        """
        inp: [num_agents*batch_size, dim_o]
        state: [num_agents*batch_size, dim_h]
        goals: [num_agents*batch_size, 2], assigned goals for agents
        mask: [batch_size, 1], mask for actions
        
        """
        # å¤„ç†è§‚æµ‹å’Œç›®æ ‡ï¼Œå¾—åˆ°æ–°çš„è¾“å…¥
        new_inp = self.data_processing_low_level(inp, goals)  

        # å‰å‘ä¼ æ’­
        x = self.low_agent_encoder(new_inp)  # should be [batch_size, h_dim]
        value = self._low_value(x)  # should be [batch_size, 1]

        # é‡‡æ ·åŠ¨ä½œ
        dist = self.dist(self._low_policy(x))
        if deterministic:
            action = dist.mode()
        else:
            action = dist.sample()
        action_log_probs = dist.log_probs(action).view(-1,1)

        return value, action, action_log_probs

    def evaluate_high_actions(self, env_states, obs, masks_batch,
                          critic_maps, critic_nodes, goals, tasks, 
                          ego_nodes, explore_nodes, 
                          landmark_datas, landmark_masks, landmark_nodes,
                          teammate_nodes, teammate_masks,
                          agent_ids):
        """
        è¯„ä¼°ç»™å®šé«˜å±‚åŠ¨ä½œçš„log_probã€ç†µå’Œä»·å€¼ï¼ˆç”¨äºPPOæ›´æ–°ï¼‰
        
        Args:
            env_states: [batch, env_dim]
            obs: [batch, obs_dim] - ç”¨äºæå–æ™ºèƒ½ä½“ä½ç½®
            masks_batch: [batch, 1] - åŠ¨ä½œæ©ç 
            critic_maps: [batch, 3, H, W] - ç”¨äºcritic
            critic_nodes: [batch, num_agents, 4] - ç”¨äºcritic
            goals: [batch, 2] - å·²é€‰æ‹©çš„ç›®æ ‡ä½ç½®ï¼ˆä¸–ç•Œåæ ‡ï¼‰
            tasks: [batch, 1] - å·²é€‰æ‹©çš„ä»»åŠ¡ç±»å‹ï¼ˆ0=explore, 1=landmarkï¼‰
            ego_nodes: [batch, 5]
            explore_nodes: [batch, K, 4]
            landmark_datas: [batch, num_landmarks, 4]
            landmark_masks: [batch, num_landmarks, 1]
            landmark_nodes: [batch, Max_L, 4]
            teammate_nodes: [batch, num_agents, 5]
            teammate_masks: [batch, num_agents, 1]
            agent_ids: [batch, 1] - æ™ºèƒ½ä½“ID
            
        Returns:
            high_values: [batch, 1] - çŠ¶æ€ä»·å€¼
            node_log_probs: [batch, 1] - ç»™å®šèŠ‚ç‚¹é€‰æ‹©çš„logæ¦‚ç‡
            node_entropy: [batch, 1] - èŠ‚ç‚¹é€‰æ‹©åˆ†å¸ƒçš„ç†µ
        """
        batch_size = env_states.size(0)
        num_agents = self.num_agents
        K = explore_nodes.size(1)  # exploreèŠ‚ç‚¹æ•°é‡

        # =====================================================
        # 1. é‡å»ºGraph nodeså’ŒEdges
        # =====================================================

        # è®¡ç®—è¾¹ç‰¹å¾
        ego_to_explore_edges, ego_to_landmark_edges, ego_to_landmark_edge_masks = self.get_edge_features(
            explore_nodes=explore_nodes,
            landmark_nodes=landmark_nodes,
            landmark_node_masks=landmark_masks,
            norm=False,
            max_distance=2.8
        )

        # =====================================================
        # 2. å¤ç”¨ get_high_level_goal çš„é€»è¾‘æ„å»ºèŠ‚ç‚¹åˆ†å¸ƒ
        # =====================================================
        B = batch_size
        
        # 2.1 èŠ‚ç‚¹å’Œè¾¹çš„ç‰¹å¾åµŒå…¥
        ego_node_feats = self.ego_node_encoder(ego_nodes)  # [B, 64]
        
        # 2.2 ExploreèŠ‚ç‚¹åµŒå…¥
        explore_node_feats = self.explore_node_encoder(
            explore_nodes.view(B * K, -1)
        ).view(B, K, -1)  # [B, K, 64]
        
        # 2.3 LandmarkèŠ‚ç‚¹GATèšåˆ
        max_L = landmark_nodes.size(1)  # [B, Max_L, 4]

        # ä¸€æ¬¡æ€§ç¼–ç æ‰€æœ‰ landmark èŠ‚ç‚¹ [B, Max_L, 4] -> [B, Max_L, 64]
        landmark_node_feats = self.landmark_node_encoder(
            landmark_nodes.view(B * max_L, -1)
        ).view(B, max_L, -1)  # [B, Max_L, 64]

        # landmark å’Œ é˜Ÿå‹èŠ‚ç‚¹è¿›è¡Œèšåˆ
        landmark_node_agg_feats = self.landmark_team_gat(
            ego_nodes[:, :2],  # åªä¼ é€’ ego çš„ä½ç½®ç”¨äºè®¡ç®—ç›¸å¯¹ä½ç½®
            teammate_nodes, teammate_masks,
            landmark_nodes, landmark_masks, landmark_node_feats
        )  # [B, Max_L, 64]

        # 2.4 å¯¹exploreå’ŒlandmarkèŠ‚ç‚¹ç‰¹å¾è¿›è¡Œçº¿æ€§å˜æ¢å’ŒLayerNorm
        explore_node_feats = self.explore_node_linear(
            explore_node_feats.view(B * K, -1)
        ).view(B, K, -1)  # [B, K, 64]

        explore_node_feats = self.linear_ln(explore_node_feats)  # LayerNorm

        landmark_node_feats = self.landmark_node_linear(
            landmark_node_agg_feats.view(B * max_L, -1)
        ).view(B, max_L, -1)  # [B, Max_L, 64]

        landmark_node_feats = self.linear_ln(landmark_node_feats)  # LayerNorm

        # 2.5 ä¸€æ¬¡æ€§ç¼–ç æ‰€æœ‰ landmark è¾¹ [B, Max_L, 3] -> [B, Max_L, 32]
        landmark_edge_feats = self.edge_encoder(
            ego_to_landmark_edges.view(B * max_L, -1)
        ).view(B, max_L, -1)  # [B, Max_L, 32]

        explore_edge_feats = self.edge_encoder(
            ego_to_explore_edges.view(B * K, -1)
        ).view(B, K, -1)  # [B, K, 32]
        
        # 2.2 æ„å»ºç»Ÿä¸€çš„å€™é€‰èŠ‚ç‚¹é›†åˆ
        total_nodes = K + max_L
        
        q = self.q_proj(ego_node_feats).unsqueeze(1)  # [B, 1, 64]
        
        unified_k = torch.zeros(B, total_nodes, self.attn_dim, device=ego_node_feats.device)
        unified_v = torch.zeros(B, total_nodes, self.attn_dim, device=ego_node_feats.device)
        unified_mask = torch.zeros(B, total_nodes, device=ego_node_feats.device, dtype=torch.bool)
        unified_relative_pos = torch.zeros(B, total_nodes, 2, device=ego_node_feats.device)

        # èŠ‚ç‚¹ç±»å‹æ ‡ç­¾: 0=explore, 1=landmark
        node_type_labels = torch.zeros(B, total_nodes, device=ego_node_feats.device, dtype=torch.long)
        
        # å¡«å…… explore èŠ‚ç‚¹
        explore_kv = torch.cat([explore_node_feats, explore_edge_feats], dim=-1)
        unified_k[:, :K, :] = self.k_proj(explore_kv.view(B * K, -1)).view(B, K, -1)
        unified_v[:, :K, :] = self.v_proj(explore_kv.view(B * K, -1)).view(B, K, -1)
        unified_mask[:, :K] = True  # explore èŠ‚ç‚¹å…¨éƒ¨æœ‰æ•ˆ
        unified_relative_pos[:, :K, :] = explore_nodes[:, :, :2]  # ç›¸å¯¹åæ ‡
        node_type_labels[:, :K] = 0  # explore ç±»å‹
        
        # å¡«å…… landmark èŠ‚ç‚¹ (ç´¢å¼• K ~ K+max_L-1)
        # æ‹¼æ¥èŠ‚ç‚¹å’Œè¾¹ç‰¹å¾ [B, Max_L, 64] + [B, Max_L, 32] -> [B, Max_L, 96]
        lm_kv = torch.cat([landmark_node_feats, landmark_edge_feats], dim=-1)
        # æŠ•å½±ä¸º K/V [B, Max_L, 96] -> [B, Max_L, 64]
        unified_k[:, K:K+max_L, :] = self.k_proj(lm_kv.view(B * max_L, -1)).view(B, max_L, -1)
        unified_v[:, K:K+max_L, :] = self.v_proj(lm_kv.view(B * max_L, -1)).view(B, max_L, -1)
        
        # æ„å»ºæœ‰æ•ˆæ©ç ï¼šmask æœ‰æ•ˆ ä¸” æœªè¢«è¿½è¸ª
        valid_mask = landmark_masks[:, :, 0] > 0.5  # [B, Max_L]
        not_targeted = landmark_nodes[:, :, 3] < 0.5    # [B, Max_L]
        combined_mask = valid_mask & not_targeted    # [B, Max_L]
        
        # å¦‚æœvalid_maskæŸè¡Œå…¨éƒ¨æœ‰æ•ˆï¼Œåˆ™è¯´æ˜å·²ç»æ‰¾åˆ°å…¨éƒ¨landmarkï¼Œåˆ™è¯¥æ ·æœ¬ä¸éœ€è¦æ¢ç´¢èŠ‚ç‚¹
        all_landmarks_found = valid_mask.all(dim=1)  # [B] bool tensor
        if all_landmarks_found.any():
            unified_mask[all_landmarks_found, :K] = False          
        
        unified_mask[:, K:K+max_L] = combined_mask
        unified_relative_pos[:, K:K+max_L, :] = landmark_nodes[:, :, :2]
        node_type_labels[:, K:K+max_L] = 1
        
        # 3. æ³¨æ„åŠ›æœºåˆ¶
        scale = math.sqrt(self.attn_dim)
        attn_scores = (q @ unified_k.transpose(1, 2)) / scale
        attn_scores = attn_scores.masked_fill(~unified_mask.unsqueeze(1), -1e9)
        
        # Softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attn_weights = torch.softmax(attn_scores, dim=-1)  # [B, 1, total_nodes]
        
        # æ³¨æ„åŠ›åŠ æƒæ±‚å’Œ
        context = (attn_weights @ unified_v).squeeze(1)  # [B, 64]
        
        # 2.4 èŠ‚ç‚¹é€‰æ‹©åˆ†å¸ƒ
        selection_logits = self.node_selection_head(unified_v).squeeze(-1)  # [B, total_nodes]
        
        # åº”ç”¨ mask
        selection_logits = selection_logits.masked_fill(~unified_mask, -1e9)
        
        # æ„å»ºåˆ†ç±»åˆ†å¸ƒ
        node_dist = TorchCategorical(logits=selection_logits)
        
        # =====================================================
        # 3. æ‰¾åˆ°ç»™å®š goal å¯¹åº”çš„èŠ‚ç‚¹ç´¢å¼•
        # =====================================================
        # å°†ç»™å®šçš„ goals (ä¸–ç•Œåæ ‡) è½¬æ¢ä¸ºç›¸å¯¹åæ ‡
        ego_pos = ego_nodes[:, :2]  # [B, 2]
        goals_relative = goals - ego_pos  # [B, 2]
        
        # è®¡ç®— goals_relative åˆ°æ‰€æœ‰èŠ‚ç‚¹çš„è·ç¦»
        # unified_relative_pos: [B, total_nodes, 2]
        # goals_relative: [B, 2] -> [B, 1, 2]
        dists = torch.norm(unified_relative_pos - goals_relative.unsqueeze(1), dim=-1)  # [B, total_nodes]
        
        # åªåœ¨æœ‰æ•ˆèŠ‚ç‚¹ä¸­æŸ¥æ‰¾æœ€è¿‘çš„
        dists_masked = dists.masked_fill(~unified_mask, float('inf'))
        selected_idx = torch.argmin(dists_masked, dim=-1)  # [B]
        
        # =====================================================
        # 4. è®¡ç®— log_prob å’Œ entropy
        # =====================================================
        node_log_probs = node_dist.log_prob(selected_idx).unsqueeze(-1)  # [B, 1]
        node_entropy = node_dist.entropy().unsqueeze(-1)  # [B, 1]
        
        # =====================================================
        # 5. è®¡ç®— Critic ä»·å€¼
        # =====================================================
        # è®¡ç®—æ‰€æœ‰æ™ºèƒ½ä½“çš„ä»·å€¼ [B, num_agents]
        all_values = self.get_high_value(critic_maps, critic_nodes)  # [B, num_agents]
        
        # æ ¹æ® agent_ids é€‰æ‹©å¯¹åº”çš„ä»·å€¼
        agent_ids_flat = agent_ids.squeeze(-1)  # [B]
        batch_indices = torch.arange(B, device=env_states.device)
        high_values = all_values[batch_indices, agent_ids_flat].unsqueeze(-1)  # [B, 1]

        return (high_values, node_log_probs, node_entropy)
    
    def evaluate_low_actions(self, inp, goals, action):
        new_inp = self.data_processing_low_level(inp, goals)
        x = self.low_agent_encoder(new_inp)
        value = self._low_value(x)
        dist = self.dist(self._low_policy(x))
        action_log_probs = dist.log_probs(action)
        dist_entropy = dist.entropy().mean()
        
        return value, action_log_probs, dist_entropy
    
    def get_low_value(self, inp, goals):
        new_inp = self.data_processing_low_level(inp, goals)
        x = self.low_agent_encoder(new_inp)
        value = self._low_value(x)
        return value

    def _world_to_grid_torch(self, world_xy: torch.Tensor, H: int, W: int) -> torch.Tensor:
        """
        world_xy: [..., 2] in continuous world coords (x,y), assumed in [-arena_size/2, arena_size/2]
        return:  [..., 2] grid indices (i,j) as int64, clipped to [0..H-1/W-1]
        """
        world_min = -1.0
        cell_size_x = 2.0 / float(H)
        cell_size_y = 2.0 / float(W)

        x = world_xy[..., 0]
        y = world_xy[..., 1]

        i = torch.floor((x - world_min) / cell_size_x)
        j = torch.floor((y - world_min) / cell_size_y)

        i = i.clamp(0, H - 1)
        j = j.clamp(0, W - 1)

        return torch.stack([i, j], dim=-1).long()
    
    def _grid_to_world_torch(self, grid_ij: torch.Tensor, H: int, W: int) -> torch.Tensor:
        """
        grid_ij: [..., 2] grid indices (i,j) as int64
        return: [..., 2] in continuous world coords (x,y), in [-arena_size/2, arena_size/2]
        """
        world_min = -1.0
        cell_size_x = 2.0 / float(H)
        cell_size_y = 2.0 / float(W)

        i = grid_ij[..., 0].float()
        j = grid_ij[..., 1].float()

        x = world_min + (i + 0.5) * cell_size_x
        y = world_min + (j + 0.5) * cell_size_y

        return torch.stack([x, y], dim=-1)  

class MultiHeadAttention(nn.Module):
    # taken from https://github.com/wouterkool/attention-tsp/blob/master/graph_encoder.py
    def __init__(
            self,
            n_heads,
            input_dim,
            embed_dim=None,
            val_dim=None,
            key_dim=None
    ):
        super(MultiHeadAttention, self).__init__()

        if val_dim is None:
            assert embed_dim is not None, "Provide either embed_dim or val_dim"
            val_dim = embed_dim // n_heads
        if key_dim is None:
            key_dim = val_dim

        self.n_heads = n_heads
        self.input_dim = input_dim
        self.embed_dim = embed_dim
        self.val_dim = val_dim
        self.key_dim = key_dim

        self.norm_factor = 1 / math.sqrt(key_dim)  # See Attention is all you need

        self.W_query = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))
        self.W_key = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))
        self.W_val = nn.Parameter(torch.Tensor(n_heads, input_dim, val_dim))

        if embed_dim is not None:
            self.W_out = nn.Parameter(torch.Tensor(n_heads, key_dim, embed_dim))

        self.init_parameters()

    def init_parameters(self):

        for param in self.parameters():
            stdv = 1. / math.sqrt(param.size(-1))
            param.data.uniform_(-stdv, stdv)

    def forward(self, q, h=None, mask=None, return_attn=False):
        """
        :param q: queries (batch_size, n_query, input_dim)
        :param h: data (batch_size, graph_size, input_dim)
        :param mask: mask (batch_size, n_query, graph_size) or viewable as that (i.e. can be 2 dim if n_query == 1)
        Mask should contain 1 if attention is not possible (i.e. mask is negative adjacency)
        :return:
        """
        if h is None:
            h = q  # compute self-attention

        # h should be (batch_size, graph_size, input_dim)
        batch_size, graph_size, input_dim = h.size()
        n_query = q.size(1)
        assert q.size(0) == batch_size
        assert q.size(2) == input_dim
        assert input_dim == self.input_dim, "Wrong embedding dimension of input"

        hflat = h.contiguous().view(-1, input_dim)
        qflat = q.contiguous().view(-1, input_dim)

        # last dimension can be different for keys and values
        shp = (self.n_heads, batch_size, graph_size, -1)
        shp_q = (self.n_heads, batch_size, n_query, -1)

        # Calculate queries, (n_heads, n_query, graph_size, key/val_size)
        Q = torch.matmul(qflat, self.W_query).view(shp_q)
        # Calculate keys and values (n_heads, batch_size, graph_size, key/val_size)
        K = torch.matmul(hflat, self.W_key).view(shp)
        V = torch.matmul(hflat, self.W_val).view(shp)

        # Calculate compatibility (n_heads, batch_size, n_query, graph_size)
        compatibility = self.norm_factor * torch.matmul(Q, K.transpose(2, 3))
        # Optionally apply mask to prevent attention
        if mask is not None:
            mask = mask.contiguous().view(1, batch_size, n_query, graph_size).expand_as(compatibility)
            compatibility[mask] = -math.inf

        attn = F.softmax(compatibility, dim=-1)

        # If there are nodes with no neighbours then softmax returns nan so we fix them to 0
        if mask is not None:
            attnc = attn.clone()
            attnc[mask] = 0
            attn = attnc

        heads = torch.matmul(attn, V)

        out = torch.mm(
            heads.permute(1, 2, 0, 3).contiguous().view(-1, self.n_heads * self.val_dim),
            self.W_out.view(-1, self.embed_dim)
        ).view(batch_size, n_query, self.embed_dim)
        
        if return_attn:
            return out, attn
        return out
